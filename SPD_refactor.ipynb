{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb33f86f-c501-4819-87f8-a68149fac2c2",
   "metadata": {},
   "source": [
    "# SPD Refector\n",
    "\n",
    "My first implementation was slow and poorly-organized. I'm reimplementing it with the following changes:\n",
    "\n",
    "1. Make subcomponents their own modules\n",
    "2. Make importance predictors their own modules\n",
    "3. Vectorize masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba9ac016-9a9d-475c-ac1d-d17c424c2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import einops\n",
    "import typing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "762ffe7b-b5db-4b0e-b3d3-a73627f0ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\"\n",
    "CONFIG = {\n",
    "    \"num_layers\": 1,\n",
    "    \"pre_embed_size\": 100,\n",
    "    \"in_size\": 1000,\n",
    "    \"hidden_size\": 50,\n",
    "    \"subcomponents_per_layer\": 30, \n",
    "    \"beta_1\": 1.0, \n",
    "    \"beta_2\": 1.0, \n",
    "    \"beta_3\": 0.1, \n",
    "    \"causal_imp_min\": 1.0, \n",
    "    \"num_mask_samples\": 20,\n",
    "    \"importance_mlp_size\": 5,\n",
    "}\n",
    "TRAIN_CONFIG = {\n",
    "    \"lr\": 8e-4,\n",
    "    \"lr_step_size\": 4,\n",
    "    \"lr_gamma\": 0.5,\n",
    "}\n",
    "BATCH_SIZE=128\n",
    "NUM_EPOCHS=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bd5934db-ae74-49aa-8bf1-2fe24f386d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 107.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg MSE loss = 10775.810246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 243.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg MSE loss = 2.699132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 245.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: avg MSE loss = 0.914385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 199.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg MSE loss = 0.447380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 211.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: avg MSE loss = 0.249607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 245.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: avg MSE loss = 0.150474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 212.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: avg MSE loss = 0.096076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 241.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: avg MSE loss = 0.064693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 243.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: avg MSE loss = 0.045993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 211.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: avg MSE loss = 0.034528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 226.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: avg MSE loss = 0.027398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 241.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: avg MSE loss = 0.022937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 239.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: avg MSE loss = 0.020132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 238.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: avg MSE loss = 0.018385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 195.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: avg MSE loss = 0.017302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 223.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: avg MSE loss = 0.016653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 248.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: avg MSE loss = 0.016272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 198.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: avg MSE loss = 0.016069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 215.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: avg MSE loss = 0.015973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 242.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: avg MSE loss = 0.015942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ToyResidMLP(nn.Module):\n",
    "    def __init__(self, config, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        # Initialize Weights for the\n",
    "        self.num_layers, self.pre_embed_size, self.in_size, self.hidden_size = config[\"num_layers\"], config[\"pre_embed_size\"], config[\"in_size\"], config[\"hidden_size\"]\n",
    "        self.device = device\n",
    "        self.W_embed = nn.Parameter(torch.empty((self.pre_embed_size, self.in_size)))\n",
    "        self.W_unembed = nn.Parameter(torch.empty((self.in_size, self.pre_embed_size)))\n",
    "        self.W_in = nn.ParameterList([torch.empty((self.in_size, self.hidden_size), device=device) for i in range(self.num_layers)])\n",
    "        self.W_out = nn.ParameterList([torch.empty((self.hidden_size, self.in_size), device=device) for i in range(self.num_layers)])\n",
    "        self.b = nn.ParameterList([torch.zeros((self.hidden_size,), device=device) for i in range(self.num_layers)])\n",
    "\n",
    "        for param in [self.W_embed, self.W_unembed] + list(self.W_in) + list(self.W_out): \n",
    "            nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        \n",
    "        assert x.shape[1] == self.pre_embed_size, f\"Input shape {x.shape[0]} does not match model's accepted size {self.pre_embed_size}\"\n",
    "        # embed \n",
    "        x_resid = torch.einsum(\"np,pi->ni\", x.clone(), self.W_embed)\n",
    "        N, D = x_resid.shape\n",
    "\n",
    "        for l in range(self.num_layers):\n",
    "            hidden = F.relu(torch.einsum(\"nd,dh -> nh\", x_resid, self.W_in[l]) + self.b[l])\n",
    "            layer_out = torch.einsum(\"nh,hd -> nd\", hidden, self.W_out[l])\n",
    "            x_resid = x_resid + layer_out\n",
    "        # am I supposed to have a embed and out?\n",
    "        x_out = torch.einsum(\"ni,ip->np\", x_resid, self.W_unembed) \n",
    "        return x_out\n",
    "\n",
    "\n",
    "# I vibecoded this originally, and regretted it. \n",
    "class SparseAutoencoderDataset(Dataset):\n",
    "    def __init__(self, in_dim=100, n_samples=10000, sparsity=0.9, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.n_samples = n_samples\n",
    "        self.device = device\n",
    "\n",
    "        # Pre-generate all samples\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            x = np.random.uniform(-1, 1, size=(in_dim))\n",
    "            mask = np.random.rand(in_dim) > sparsity  # 1-sparsity fraction will be nonzero\n",
    "            x = x * mask\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "            \n",
    "            target = F.relu(x)\n",
    "            \n",
    "            self.inputs.append(x)\n",
    "            self.targets.append(target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "def train_toy_resid_mlp(\n",
    "    model,\n",
    "    dataloader,\n",
    "    lr=1e-3,\n",
    "    num_epochs=10,\n",
    "    device=\"cpu\",\n",
    "    print_every=1\n",
    "):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for x, y in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}: avg MSE loss = {avg_loss:.6f}\")\n",
    "\n",
    "\n",
    "## LLM-Generated Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    device=DEVICE\n",
    "    config=CONFIG\n",
    "    \n",
    "    dataset = SparseAutoencoderDataset(\n",
    "        in_dim=100,\n",
    "        n_samples=15000,\n",
    "        sparsity=0.9,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    print(device)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    # Model\n",
    "    toy_model = ToyResidMLP(config, device=device)\n",
    "    # Train\n",
    "    train_toy_resid_mlp(toy_model, dataloader, lr=8e-2, num_epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29df3ae6-4c76-4b98-9f27-1960aecd8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subcomponent(nn.Module): \n",
    "    # Subcomponents approximate a in_dim x out_dim matrix with c components\n",
    "    def __init__(self, shape, num_components, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.in_dims = shape[0]\n",
    "        self.out_dims = shape[1]\n",
    "        self.shape = shape\n",
    "        self.C = num_components\n",
    "        self.device=device\n",
    "        \n",
    "        self.V = nn.Parameter(torch.empty((self.in_dims, self.C), device=device))\n",
    "        self.U = nn.Parameter(torch.empty((self.C, self.out_dims), device=device))\n",
    "        \n",
    "        nn.init.xavier_normal_(self.V)\n",
    "        nn.init.xavier_normal_(self.U)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # self is shape (in_dims, C), (C, out_dims) -> (in_dims, out_dims)\n",
    "        # x is shape (N, in_dims)\n",
    "        # mask is shape (N, C)\n",
    "        N, in_dims = x.shape\n",
    "        \n",
    "        if mask is None: \n",
    "            activations = x @ self.V\n",
    "        else: \n",
    "            activations = x @ self.V * mask\n",
    "        out = activations @ self.U\n",
    "        \n",
    "        return out, activations\n",
    "\n",
    "    def return_weights(self): \n",
    "        weights = self.V @ self.U\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "338108c5-af4d-416b-b2fc-822616c07e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportancePredictor(nn.Module):\n",
    "    def __init__(self, hidden_size, num_components, device=\"cuda\"): \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.C = num_components\n",
    "        self.device = device\n",
    "\n",
    "        # These are techncially (C, hidden, 1) and (C, 1, hidden) \n",
    "        # but squeezing and unsqueezing is less efficient\n",
    "        self.W_gate_in = nn.Parameter(torch.empty((self.C, hidden_size), device=device))\n",
    "        self.W_gate_out = nn.Parameter(torch.empty((self.C, hidden_size), device=device))\n",
    "        \n",
    "        # I think that nonzero biases will be easier to learn \n",
    "        self.b_in = nn.Parameter(torch.zeros((self.C, hidden_size), device=device) + 0.1)\n",
    "        self.b_out = nn.Parameter(torch.zeros((self.C,), device=device) + 0.1) # technically shape (C, 1)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.W_gate_in)\n",
    "        nn.init.xavier_normal_(self.W_gate_out)\n",
    "\n",
    "\n",
    "    def forward(self, subcomponent_activations):\n",
    "        # Activations are shape (N, C) (see Subcomponent.forward)\n",
    "        # In_weights are shape (C, hidden_size) \n",
    "        hidden = F.gelu(torch.einsum(\"nc,cs->ncs\", subcomponent_activations, self.W_gate_in) + self.b_in)\n",
    "        prediction_out = torch.einsum(\"ncs,cs->nc\", hidden, self.W_gate_out) + self.b_out\n",
    "\n",
    "        return prediction_out # now shape (N,C). Unsqueeze to get (N,C,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "504fc5cc-c9a3-4738-ae7e-971391b81ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSubcomponentLayer(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_components, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # \"in matrix\" since `in` is a builtin thing\n",
    "        self.in_mat = Subcomponent((embed_size, hidden_size), num_components, device=device)\n",
    "        self.out_mat = Subcomponent((hidden_size, embed_size), num_components, device=device)\n",
    "        self.bias = nn.Parameter(torch.zeros((1, hidden_size,), device=device)) \n",
    "\n",
    "    def forward(self, x, masks=None):\n",
    "        # Mask should be dict {in: (N, C), out: (N,C)}\n",
    "        if masks is None: \n",
    "            masks={\"in\": None, \"out\": None} # So that mask[0] won't break; instead will pass None in which is ok\n",
    "        mat_output, activs_in = self.in_mat(x, masks[\"in\"])\n",
    "        hidden= F.relu(mat_output + self.bias)\n",
    "        out, activs_out = self.out_mat(hidden, masks[\"out\"])\n",
    "        return out, {\"in\": activs_in, \"out\": activs_out}\n",
    "\n",
    "    def return_weights_layer(self):\n",
    "        return {\"in\": self.in_mat.return_weights(), \"out\": self.out_mat.return_weights()}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dbd28d5-2bb2-4f79-a155-7305ca013d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPDModelMLP(nn.Module): \n",
    "    def __init__(self, target_model, config, device=\"cuda\"): \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        object.__setattr__(self, \"target_model\", target_model) # sets pointer to target_model without registering its parameters as subsidiary\n",
    "        \n",
    "        # Unpack Config\n",
    "        self.C, self.num_layers, self.pre_embed_size, self.embed_size, self.hidden_size, self.imp_hidden_size = config[\"subcomponents_per_layer\"], config[\"num_layers\"], config[\"pre_embed_size\"], config[\"in_size\"], config[\"hidden_size\"], config[\"importance_mlp_size\"]\n",
    "        self.hypers = dict(list(config.items())[4:])\n",
    "        self.num_matrices = self.num_layers * 2 + 2\n",
    "        self.P = sum(p.numel() for p in self.target_model.parameters())\n",
    "\n",
    "\n",
    "        # Define weights/subcomponents \n",
    "        self.embed = Subcomponent((self.pre_embed_size, self.embed_size), self.C, device=device)\n",
    "        self.unembed = Subcomponent((self.embed_size, self.pre_embed_size), self.C, device=device)\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            MLPSubcomponentLayer(self.embed_size, self.hidden_size, self.C, device=device) for _ in range(self.num_layers)\n",
    "        )\n",
    "        \n",
    "        # Define Importance Predictors\n",
    "        # You can index importance predictors via imp_pred_ers[layer][\"in\"/\"out\"]\n",
    "        self.imp_pred_ers = nn.ModuleList(\n",
    "            [nn.ModuleDict({\n",
    "                name: ImportancePredictor(self.imp_hidden_size, self.C, device=device) for name in [\"in\", \"out\"]                    \n",
    "            }) for l in range(self.num_layers)] +\n",
    "            [nn.ModuleDict({\n",
    "                name: ImportancePredictor(self.imp_hidden_size, self.C, device=device) for name in [\"embed\", \"unembed\"]                    \n",
    "            })]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, masks=None, return_activs_weights=False):\n",
    "        # Create the masks object so that dict keys never break (simplifies code)\n",
    "        if masks is None:\n",
    "            layer_masks = [\n",
    "                { name: None for name in (\"in\", \"out\") }\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "            end_masks = { name: None for name in (\"embed\", \"unembed\") }\n",
    "            masks = layer_masks + [end_masks]\n",
    "\n",
    "        \n",
    "        activations = []\n",
    "\n",
    "        # regular forward pass\n",
    "        x, embed_activs = self.embed(x, masks[-1][\"embed\"])\n",
    "        for l in range(self.num_layers):\n",
    "            x, layer_activations = self.layers[l](x, masks=masks[l])\n",
    "            activations.append(layer_activations)\n",
    "            \n",
    "        x, unembed_activs = self.unembed(x, masks[-1][\"unembed\"])\n",
    "        activations.append({\"embed\": embed_activs, \"unembed\": unembed_activs})             \n",
    "        \n",
    "        \n",
    "        if not return_activs_weights:\n",
    "            return x\n",
    "        else:\n",
    "            weights = []\n",
    "\n",
    "            for l in range(self.num_layers):\n",
    "                weights.append(self.layers[l].return_weights_layer())\n",
    "            weights.append({\"embed\": self.embed.return_weights(), \"unembed\": self.unembed.return_weights()})\n",
    "            \n",
    "            return x, activations, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8539ebae-274f-4d8c-83a5-c57ce0cbfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the hard sigmoid activation function as described in the paper:\n",
    "        σ_H(x) = 0 if x <= 0\n",
    "               = x if 0 < x < 1\n",
    "               = 1 if x >= 1\n",
    "    This is equivalent to: torch.clamp(x, min=0.0, max=1.0)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Clamp values between 0 and 1\n",
    "        return torch.clamp(x, min=0.0, max=1.0)\n",
    "\n",
    "\n",
    "class LowerLeakyHardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Lower-leaky hard sigmoid: σH,lower(x)\n",
    "    - 0.01*x if x <= 0 (leaky below 0)\n",
    "    - x if 0 <= x <= 1 (linear in middle)  \n",
    "    - 1 if x >= 1 (saturated above 1)\n",
    "    \n",
    "    Used for forward pass masks in stochastic reconstruction losses.\n",
    "    \"\"\"\n",
    "    def __init__(self, leak_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.leak_slope = leak_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(\n",
    "            x <= 0, \n",
    "            self.leak_slope * x,\n",
    "            torch.where(\n",
    "                x >= 1,\n",
    "                torch.ones_like(x),\n",
    "                x\n",
    "            )\n",
    "        )\n",
    "\n",
    "class UpperLeakyHardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Upper-leaky hard sigmoid: σH,upper(x)  \n",
    "    - 0 if x <= 0 (hard cutoff below 0)\n",
    "    - x if 0 <= x <= 1 (linear in middle)\n",
    "    - 1 + 0.01*(x-1) if x >= 1 (leaky above 1)\n",
    "    \n",
    "    Used for importance loss computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, leak_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.leak_slope = leak_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(\n",
    "            x <= 0,\n",
    "            torch.zeros_like(x), \n",
    "            torch.where(\n",
    "                x >= 1,\n",
    "                1 + self.leak_slope * (x - 1),\n",
    "                x\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bf25bf6a-c788-4186-8190-b7d136c30949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spd(spd_model, dataloader, train_config, num_epochs=1):\n",
    "\n",
    "    # SPD model is a model\n",
    "    spd_model.train()\n",
    "    print(f\"Training on device {spd_model.device}\")\n",
    "    optimizer = torch.optim.AdamW(spd_model.parameters(), lr = train_config[\"lr\"])\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, \n",
    "        step_size=train_config[\"lr_step_size\"],   # e.g. every 4 epochs\n",
    "        gamma=train_config[\"lr_gamma\"]      # multiply LR by 0.5 each time\n",
    "    )\n",
    "    \n",
    "    # P = sum(p.numel() for p in spd_model.target_model.parameters()) [moved into model]\n",
    "    upper_leaky_sigmoid = UpperLeakyHardSigmoid()\n",
    "    lower_leaky_sigmoid = LowerLeakyHardSigmoid()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_l_stoch_rec, total_l_stoch_rec_l, total_l_imp, total_l_faith = 0.0, 0.0, 0.0, 0.0\n",
    "        print(f\"Starting epoch {epoch+1}, lr = {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as t:\n",
    "            for batch_idx, (x,y) in enumerate(t):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                N = x.shape[0]     # x is shape N by in_size\n",
    "                S, C, P, L = spd_model.hypers[\"num_mask_samples\"], spd_model.C, spd_model.P, spd_model.num_layers\n",
    "                target_model = spd_model.target_model\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # ====== TARGET MODEL OUTPUT ======\n",
    "                with torch.no_grad(): \n",
    "                    target_out = spd_model.target_model(x)\n",
    "\n",
    "                # ====== FAITHFULNESS LOSS ======\n",
    "                spd_output, spd_activations, spd_weights = spd_model(x, return_activs_weights = True)\n",
    "                squared_error = 0\n",
    "\n",
    "                for l in range(L):\n",
    "                    in_diff = target_model.W_in[l] - spd_weights[l][\"in\"]\n",
    "                    out_diff = target_model.W_out[l] - spd_weights[l][\"out\"]\n",
    "                    squared_error_layer = torch.linalg.matrix_norm(in_diff)**2 + torch.linalg.matrix_norm(out_diff)**2 \n",
    "                    squared_error = squared_error + squared_error_layer\n",
    "\n",
    "                embed_diff = target_model.W_embed - spd_weights[-1][\"embed\"]\n",
    "                unembed_diff = target_model.W_unembed - spd_weights[-1][\"unembed\"]\n",
    "                squared_error_embed = torch.linalg.matrix_norm(embed_diff)**2 + torch.linalg.matrix_norm(unembed_diff)**2 \n",
    "                squared_error = squared_error + squared_error_embed\n",
    "\n",
    "                l_faithfulness = squared_error/P # \"Mean Squared Error\" across parameters\n",
    "                \n",
    "\n",
    "                # ===== IMPORTANCE MINIMALITY LOSS ======\n",
    "                l_importance_minimality = 0.0\n",
    "\n",
    "                pred_importances = []\n",
    "\n",
    "                imp_pred_embed = spd_model.imp_pred_ers[-1][\"embed\"](spd_activations[-1][\"embed\"])\n",
    "                imp_pred_unembed = spd_model.imp_pred_ers[-1][\"unembed\"](spd_activations[-1][\"unembed\"])\n",
    "                l_importance_minimality = l_importance_minimality + (upper_leaky_sigmoid(imp_pred_embed)** spd_model.hypers[\"causal_imp_min\"]).sum() + (upper_leaky_sigmoid(imp_pred_unembed) ** spd_model.hypers[\"causal_imp_min\"]).sum()\n",
    "                for l in range(L):\n",
    "                    imp_pred_in = spd_model.imp_pred_ers[l][\"in\"](spd_activations[l][\"in\"])\n",
    "                    imp_pred_out = spd_model.imp_pred_ers[l][\"out\"](spd_activations[l][\"out\"])\n",
    "                    pred_importances.append({\"in\": imp_pred_in, \"out\": imp_pred_out})\n",
    "                    l_importance_minimality = l_importance_minimality + (upper_leaky_sigmoid(imp_pred_in) ** spd_model.hypers[\"causal_imp_min\"]).sum()  + (upper_leaky_sigmoid(imp_pred_out)** spd_model.hypers[\"causal_imp_min\"]).sum() \n",
    "                pred_importances.append({\"embed\": imp_pred_embed, \"unembed\": imp_pred_unembed})\n",
    "\n",
    "                l_importance_minimality /= N\n",
    "\n",
    "\n",
    "                # ===== STOCHASTIC RECONSTRUCTION LOSS ======\n",
    "                l_stochastic_recon = 0.0\n",
    "                l_stochastic_recon_layerwise = 0.0\n",
    "                R = torch.rand((S, N, L+1, 2, C), device=device)\n",
    "\n",
    "                # we have activs in shape (N,C). Stack and unsqueeze(0) to (1,2,N,C) and cat along dim 1 to get (L+1,2,N,C)\n",
    "                stacked_imps = torch.cat([torch.stack((pred_importances[l][\"in\"], pred_importances[l][\"out\"])).reshape(1,2,N,C) for l in range(L)] + [torch.stack((pred_importances[-1][\"embed\"],pred_importances[-1][\"unembed\"])).reshape(1,2,N,C)])\n",
    "                # reshape (L+1, 2, N, C) -> (N, L+1, 2, C)\n",
    "                stacked_imps = torch.movedim(stacked_imps, 2, 0)\n",
    "                # Apply sigmoid and then reshape to (1, N, L+1, 2, C)\n",
    "                G = lower_leaky_sigmoid(stacked_imps).unsqueeze(0)\n",
    "                masks = G + (1-G)*R # shape (S, N, L+1, 2, C)\n",
    "                masks = masks.reshape(S*N, L+1, 2, C)\n",
    "\n",
    "                # Move masks back into layerwise structure\n",
    "                layer_masks = [\n",
    "                    { name: masks[:,l,idx,:] for name, idx in [(\"in\",0), (\"out\",1)] } for l in range(L)\n",
    "                ]\n",
    "                end_masks = { name: masks[:,l+1,idx,:] for name, idx in [(\"embed\", 0), (\"unembed\", 1)] }\n",
    "                masks_dictified = layer_masks + [end_masks]\n",
    "\n",
    "                #Tile inputs (1, N, embed_size) S times to have N_new = S * N -> (N * S, embed_size)\n",
    "                x_repeated = x.clone().unsqueeze(0).expand(S, -1, -1).reshape(S * N, -1)\n",
    "\n",
    "                # Run regular masked loss and update stochastic recon\n",
    "                masked_out = spd_model(x_repeated, masks=masks_dictified)\n",
    "                target_out_tiled = target_out.clone().unsqueeze(0).expand(S, -1, -1).reshape(S*N, -1)\n",
    "                l_stochastic_recon = l_stochastic_recon + torch.linalg.matrix_norm(target_out_tiled-masked_out)**2\n",
    "                l_stochastic_recon /= S\n",
    "\n",
    "                # ===== STOCHASTIC RECONSTRUCTION LAYERWISE LOSS ======\n",
    "                # Create Layerwise Mask constructor\n",
    "                layer_masks_none = [\n",
    "                    { name: None for name in (\"in\", \"out\") }\n",
    "                    for _ in range(L)\n",
    "                ]\n",
    "                end_masks_none = { name: None for name in (\"embed\", \"unembed\") }\n",
    "                layer_mask_constructor = layer_masks_none + [end_masks_none]\n",
    "\n",
    "                # Embed and unembed outs\n",
    "                embed_mask = layer_mask_constructor.copy()\n",
    "                embed_mask[-1][\"embed\"] = masks_dictified[-1][\"embed\"]\n",
    "                embed_out=spd_model(x_repeated, embed_mask)\n",
    "\n",
    "                unembed_mask = layer_mask_constructor.copy()\n",
    "                unembed_mask[-1][\"unembed\"] = masks_dictified[-1][\"unembed\"]\n",
    "                unembed_out=spd_model(x_repeated,unembed_mask)\n",
    "                l_stochastic_recon_layerwise = l_stochastic_recon_layerwise + torch.linalg.matrix_norm(target_out_tiled-embed_out)**2 + torch.linalg.matrix_norm(target_out_tiled-unembed_out)**2\n",
    "\n",
    "                for l in range(l): \n",
    "                    for name in [\"in\", \"out\"]:\n",
    "                        layer_mask = layer_mask_constructor.copy()\n",
    "                        layer_mask[l][name] = masks_dictified[l][name]\n",
    "                        layer_out = spd_model(x_repeated,layer_mask)\n",
    "                        l_stochastic_recon_layerwise = l_stochastic_recon_layerwise + torch.linalg.matrix_norm(target_out_tiled-embed_out)**2\n",
    "                        \n",
    "                l_stochastic_recon_layerwise /= (S * spd_model.num_matrices)\n",
    "                                \n",
    "                beta1, beta2, beta3 = spd_model.hypers[\"beta_1\"], spd_model.hypers[\"beta_2\"], spd_model.hypers[\"beta_3\"]\n",
    "\n",
    "                # Loss computations\n",
    "                loss = l_faithfulness + beta1*l_stochastic_recon + beta2*l_stochastic_recon_layerwise + beta3*l_importance_minimality\n",
    "                loss.backward()\n",
    "                optimizer.step() \n",
    "                t.set_postfix(loss=loss.item())\n",
    "            print(f\"(Last batch) Faithfulness: {l_faithfulness}, Stoch Rec: {l_stochastic_recon}, Stoch Rec Layerwise: {l_stochastic_recon_layerwise}, Importance Min: {l_importance_minimality}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "462a42a9-0311-4293-9a83-8b3093cb0702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device mps\n",
      "Starting epoch 1, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 47.14it/s, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0020853562746196985, Stoch Rec: 0.8647415041923523, Stoch Rec Layerwise: 0.3849048614501953, Importance Min: 0.15678267180919647\n",
      "Starting epoch 2, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 49.19it/s, loss=1.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.002012841636314988, Stoch Rec: 0.7959372401237488, Stoch Rec Layerwise: 0.3166409134864807, Importance Min: 0.03770322725176811\n",
      "Starting epoch 3, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 49.94it/s, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0019466744270175695, Stoch Rec: 0.8418064117431641, Stoch Rec Layerwise: 0.2982615530490875, Importance Min: 0.05962591990828514\n",
      "Starting epoch 4, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 48.37it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.001888036960735917, Stoch Rec: 0.8582914471626282, Stoch Rec Layerwise: 0.3030981123447418, Importance Min: 0.11846653372049332\n",
      "Starting epoch 5, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 48.82it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0018354130443185568, Stoch Rec: 0.8585014343261719, Stoch Rec Layerwise: 0.30919575691223145, Importance Min: 0.14506344497203827\n",
      "Starting epoch 6, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 48.94it/s, loss=0.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0017866144189611077, Stoch Rec: 0.6929793953895569, Stoch Rec Layerwise: 0.24649257957935333, Importance Min: 0.1913660317659378\n",
      "Starting epoch 7, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 48.87it/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0017462731339037418, Stoch Rec: 0.9930356740951538, Stoch Rec Layerwise: 0.3165791630744934, Importance Min: 0.38594111800193787\n",
      "Starting epoch 8, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 47.41it/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0017149151535704732, Stoch Rec: 0.7765235900878906, Stoch Rec Layerwise: 0.29021525382995605, Importance Min: 0.722955048084259\n",
      "Starting epoch 9, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 48.24it/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.001686715753749013, Stoch Rec: 0.8582080006599426, Stoch Rec Layerwise: 0.3389424681663513, Importance Min: 1.14987051486969\n",
      "Starting epoch 10, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 48.78it/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0016565141268074512, Stoch Rec: 0.728486955165863, Stoch Rec Layerwise: 0.2528616487979889, Importance Min: 1.5116595029830933\n",
      "Starting epoch 11, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 49.44it/s, loss=1.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.001639935071580112, Stoch Rec: 0.6906450986862183, Stoch Rec Layerwise: 0.2563306391239166, Importance Min: 1.6820393800735474\n",
      "Starting epoch 12, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 49.17it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0016250226181000471, Stoch Rec: 0.6643909215927124, Stoch Rec Layerwise: 0.2479252815246582, Importance Min: 2.709808111190796\n",
      "Starting epoch 13, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 48.52it/s, loss=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.001602047705091536, Stoch Rec: 0.6674121618270874, Stoch Rec Layerwise: 0.2529870569705963, Importance Min: 3.4084484577178955\n",
      "Starting epoch 14, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 49.68it/s, loss=1.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0015834869118407369, Stoch Rec: 0.687835693359375, Stoch Rec Layerwise: 0.26173466444015503, Importance Min: 4.462008953094482\n",
      "Starting epoch 15, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 49.09it/s, loss=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0015691678272560239, Stoch Rec: 0.646634042263031, Stoch Rec Layerwise: 0.25131699442863464, Importance Min: 4.746394634246826\n",
      "Starting epoch 16, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 49.45it/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0015613391296938062, Stoch Rec: 0.5961915254592896, Stoch Rec Layerwise: 0.2123069316148758, Importance Min: 5.140549182891846\n",
      "Starting epoch 17, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 48.09it/s, loss=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.001552774803712964, Stoch Rec: 0.5644382238388062, Stoch Rec Layerwise: 0.23972158133983612, Importance Min: 5.598663330078125\n",
      "Starting epoch 18, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 49.39it/s, loss=1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.001542234793305397, Stoch Rec: 0.6072854995727539, Stoch Rec Layerwise: 0.23044335842132568, Importance Min: 6.460878849029541\n",
      "Starting epoch 19, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 48.86it/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.001534105627797544, Stoch Rec: 0.5499958992004395, Stoch Rec Layerwise: 0.2130579650402069, Importance Min: 5.904991149902344\n",
      "Starting epoch 20, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 45.51it/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0015282455133274198, Stoch Rec: 0.5234001278877258, Stoch Rec Layerwise: 0.21105948090553284, Importance Min: 5.80555534362793\n",
      "Starting epoch 21, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 42.42it/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0015207236865535378, Stoch Rec: 0.46776431798934937, Stoch Rec Layerwise: 0.18135127425193787, Importance Min: 5.426008224487305\n",
      "Starting epoch 22, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:03<00:00, 36.81it/s, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.001516428543254733, Stoch Rec: 0.4723374843597412, Stoch Rec Layerwise: 0.19280461966991425, Importance Min: 6.134443283081055\n",
      "Starting epoch 23, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 43.40it/s, loss=1.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0015142298070713878, Stoch Rec: 0.5100651979446411, Stoch Rec Layerwise: 0.20373240113258362, Importance Min: 6.881712436676025\n",
      "Starting epoch 24, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 47.69it/s, loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0015123555203899741, Stoch Rec: 0.5451668500900269, Stoch Rec Layerwise: 0.22488179802894592, Importance Min: 6.857992649078369\n",
      "Starting epoch 25, lr = 8.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|███████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:02<00:00, 47.82it/s, loss=1.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Last batch) Faithfulness: 0.0015070681693032384, Stoch Rec: 0.5214187502861023, Stoch Rec Layerwise: 0.2080308496952057, Importance Min: 6.5756354331970215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = DEVICE # specified at beginning of file\n",
    "    config = CONFIG\n",
    "    train_config = TRAIN_CONFIG\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    spd_model = SPDModelMLP(toy_model, config, device)\n",
    "    train_spd(spd_model, dataloader, train_config, num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930bed83-97b8-4990-8d5c-831eb7edc612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-replication",
   "language": "python",
   "name": "interp-replication"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
