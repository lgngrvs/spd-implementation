{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb33f86f-c501-4819-87f8-a68149fac2c2",
   "metadata": {},
   "source": [
    "# SPD Refector\n",
    "\n",
    "My first implementation was slow and poorly-organized. I'm reimplementing it with the following changes:\n",
    "\n",
    "1. Make subcomponents their own modules\n",
    "2. Make importance predictors their own modules\n",
    "3. Vectorize masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9ac016-9a9d-475c-ac1d-d17c424c2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import einops\n",
    "import typing\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "762ffe7b-b5db-4b0e-b3d3-a73627f0ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\"\n",
    "CONFIG = {\n",
    "    \"num_layers\": 1,\n",
    "    \"pre_embed_size\": 100,\n",
    "    \"in_size\": 1000,\n",
    "    \"hidden_size\": 50,\n",
    "    \"subcomponents_per_layer\": 30, \n",
    "    \"beta_1\": 1.0, \n",
    "    \"beta_2\": 1.0, \n",
    "    \"beta_3\": 0.1, \n",
    "    \"causal_imp_min\": 1.0, \n",
    "    \"num_mask_samples\": 20,\n",
    "    \"importance_mlp_size\": 5,\n",
    "}\n",
    "TRAIN_CONFIG = {\n",
    "    \"lr\": 8e-3,\n",
    "    \"lr_step_size\": 8,\n",
    "    \"lr_gamma\": 0.5,\n",
    "}\n",
    "BATCH_SIZE=128\n",
    "NUM_EPOCHS=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5934db-ae74-49aa-8bf1-2fe24f386d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyResidMLP(nn.Module):\n",
    "    def __init__(self, config, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        # Initialize Weights for the\n",
    "        self.num_layers, self.pre_embed_size, self.in_size, self.hidden_size = config[\"num_layers\"], config[\"pre_embed_size\"], config[\"in_size\"], config[\"hidden_size\"]\n",
    "        self.device = device\n",
    "        self.W_embed = nn.Parameter(torch.empty((self.pre_embed_size, self.in_size)))\n",
    "        self.W_unembed = nn.Parameter(torch.empty((self.in_size, self.pre_embed_size)))\n",
    "        self.W_in = nn.ParameterList([torch.empty((self.in_size, self.hidden_size), device=device) for i in range(self.num_layers)])\n",
    "        self.W_out = nn.ParameterList([torch.empty((self.hidden_size, self.in_size), device=device) for i in range(self.num_layers)])\n",
    "        self.b = nn.ParameterList([torch.zeros((self.hidden_size,), device=device) for i in range(self.num_layers)])\n",
    "\n",
    "        for param in [self.W_embed, self.W_unembed] + list(self.W_in) + list(self.W_out): \n",
    "            nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        \n",
    "        assert x.shape[1] == self.pre_embed_size, f\"Input shape {x.shape[0]} does not match model's accepted size {self.pre_embed_size}\"\n",
    "        # embed \n",
    "        x_resid = torch.einsum(\"np,pi->ni\", x.clone(), self.W_embed)\n",
    "        N, D = x_resid.shape\n",
    "\n",
    "        for l in range(self.num_layers):\n",
    "            hidden = F.relu(torch.einsum(\"nd,dh -> nh\", x_resid, self.W_in[l]) + self.b[l])\n",
    "            layer_out = torch.einsum(\"nh,hd -> nd\", hidden, self.W_out[l])\n",
    "            x_resid = x_resid + layer_out\n",
    "        # am I supposed to have a embed and out?\n",
    "        x_out = torch.einsum(\"ni,ip->np\", x_resid, self.W_unembed) \n",
    "        return x_out\n",
    "\n",
    "\n",
    "# I vibecoded this originally, and regretted it. \n",
    "class SparseAutoencoderDataset(Dataset):\n",
    "    def __init__(self, in_dim=100, n_samples=10000, sparsity=0.9, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.n_samples = n_samples\n",
    "        self.device = device\n",
    "\n",
    "        # Pre-generate all samples\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            x = np.random.uniform(-1, 1, size=(in_dim))\n",
    "            mask = np.random.rand(in_dim) > sparsity  # 1-sparsity fraction will be nonzero\n",
    "            x = x * mask\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "            \n",
    "            target = F.relu(x)\n",
    "            \n",
    "            self.inputs.append(x)\n",
    "            self.targets.append(target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "def train_toy_resid_mlp(\n",
    "    model,\n",
    "    dataloader,\n",
    "    lr=1e-3,\n",
    "    num_epochs=10,\n",
    "    device=\"cuda\",\n",
    "    print_every=1\n",
    "):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for x, y in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}: avg MSE loss = {avg_loss:.6f}\")\n",
    "\n",
    "\n",
    "## LLM-Generated Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    device=DEVICE\n",
    "    config=CONFIG\n",
    "    \n",
    "    dataset = SparseAutoencoderDataset(\n",
    "        in_dim=100,\n",
    "        n_samples=131072,\n",
    "        sparsity=0.9,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    print(device)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    # Model\n",
    "    toy_model = ToyResidMLP(config, device=device)\n",
    "    # Train\n",
    "    train_toy_resid_mlp(toy_model, dataloader, lr=8e-2, num_epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29df3ae6-4c76-4b98-9f27-1960aecd8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subcomponent(nn.Module): \n",
    "    # Subcomponents approximate a in_dim x out_dim matrix with c components\n",
    "    def __init__(self, shape, num_components, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.in_dims = shape[0]\n",
    "        self.out_dims = shape[1]\n",
    "        self.shape = shape\n",
    "        self.C = num_components\n",
    "        self.device=device\n",
    "        \n",
    "        self.V = nn.Parameter(torch.empty((self.in_dims, self.C), device=device))\n",
    "        self.U = nn.Parameter(torch.empty((self.C, self.out_dims), device=device))\n",
    "        \n",
    "        nn.init.xavier_normal_(self.V)\n",
    "        nn.init.xavier_normal_(self.U)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # self is shape (in_dims, C), (C, out_dims) -> (in_dims, out_dims)\n",
    "        # x is shape (N, in_dims)\n",
    "        # mask is shape (N, C)\n",
    "        N, in_dims = x.shape\n",
    "        \n",
    "        if mask is None: \n",
    "            activations = x @ self.V\n",
    "        else: \n",
    "            activations = x @ self.V * mask\n",
    "        out = activations @ self.U\n",
    "        \n",
    "        return out, activations\n",
    "\n",
    "    def return_weights(self): \n",
    "        weights = self.V @ self.U\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "338108c5-af4d-416b-b2fc-822616c07e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportancePredictor(nn.Module):\n",
    "    def __init__(self, hidden_size, num_components, device=\"cuda\"): \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.C = num_components\n",
    "        self.device = device\n",
    "\n",
    "        # These are techncially (C, hidden, 1) and (C, 1, hidden) \n",
    "        # but squeezing and unsqueezing is less efficient\n",
    "        self.W_gate_in = nn.Parameter(torch.empty((self.C, hidden_size), device=device))\n",
    "        self.W_gate_out = nn.Parameter(torch.empty((self.C, hidden_size), device=device))\n",
    "        \n",
    "        # I think that nonzero biases will be easier to learn \n",
    "        self.b_in = nn.Parameter(torch.zeros((self.C, hidden_size), device=device) + 0.1)\n",
    "        self.b_out = nn.Parameter(torch.zeros((self.C,), device=device) + 0.1) # technically shape (C, 1)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.W_gate_in)\n",
    "        nn.init.xavier_normal_(self.W_gate_out)\n",
    "\n",
    "\n",
    "    def forward(self, subcomponent_activations):\n",
    "        # Activations are shape (N, C) (see Subcomponent.forward)\n",
    "        # In_weights are shape (C, hidden_size) \n",
    "        hidden = F.gelu(torch.einsum(\"nc,cs->ncs\", subcomponent_activations, self.W_gate_in) + self.b_in)\n",
    "        prediction_out = torch.einsum(\"ncs,cs->nc\", hidden, self.W_gate_out) + self.b_out\n",
    "\n",
    "        return prediction_out # now shape (N,C). Unsqueeze to get (N,C,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "504fc5cc-c9a3-4738-ae7e-971391b81ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSubcomponentLayer(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_components, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # \"in matrix\" since `in` is a builtin thing\n",
    "        self.in_mat = Subcomponent((embed_size, hidden_size), num_components, device=device)\n",
    "        self.out_mat = Subcomponent((hidden_size, embed_size), num_components, device=device)\n",
    "        self.bias = nn.Parameter(torch.zeros((1, hidden_size,), device=device)) \n",
    "\n",
    "    def forward(self, x, masks=None):\n",
    "        # Mask should be dict {in: (N, C), out: (N,C)}\n",
    "        if masks is None: \n",
    "            masks={\"in\": None, \"out\": None} # So that mask[0] won't break; instead will pass None in which is ok\n",
    "        mat_output, activs_in = self.in_mat(x, masks[\"in\"])\n",
    "        hidden= F.relu(mat_output + self.bias)\n",
    "        out, activs_out = self.out_mat(hidden, masks[\"out\"])\n",
    "        return out, {\"in\": activs_in, \"out\": activs_out}\n",
    "\n",
    "    def return_weights_layer(self):\n",
    "        return {\"in\": self.in_mat.return_weights(), \"out\": self.out_mat.return_weights()}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dbd28d5-2bb2-4f79-a155-7305ca013d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPDModelMLP(nn.Module): \n",
    "    def __init__(self, target_model, config, device=\"cuda\"): \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        object.__setattr__(self, \"target_model\", target_model) # sets pointer to target_model without registering its parameters as subsidiary\n",
    "        \n",
    "        # Unpack Config\n",
    "        self.C, self.num_layers, self.pre_embed_size, self.embed_size, self.hidden_size, self.imp_hidden_size = config[\"subcomponents_per_layer\"], config[\"num_layers\"], config[\"pre_embed_size\"], config[\"in_size\"], config[\"hidden_size\"], config[\"importance_mlp_size\"]\n",
    "        self.hypers = dict(list(config.items())[4:])\n",
    "        self.num_matrices = self.num_layers * 2 + 2\n",
    "        self.P = sum(p.numel() for p in self.target_model.parameters())\n",
    "\n",
    "\n",
    "        # Define weights/subcomponents \n",
    "        self.embed = Subcomponent((self.pre_embed_size, self.embed_size), self.C, device=device)\n",
    "        self.unembed = Subcomponent((self.embed_size, self.pre_embed_size), self.C, device=device)\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            MLPSubcomponentLayer(self.embed_size, self.hidden_size, self.C, device=device) for _ in range(self.num_layers)\n",
    "        )\n",
    "        \n",
    "        # Define Importance Predictors\n",
    "        # You can index importance predictors via imp_pred_ers[layer][\"in\"/\"out\"]\n",
    "        self.imp_pred_ers = nn.ModuleList(\n",
    "            [nn.ModuleDict({\n",
    "                name: ImportancePredictor(self.imp_hidden_size, self.C, device=device) for name in [\"in\", \"out\"]                    \n",
    "            }) for l in range(self.num_layers)] +\n",
    "            [nn.ModuleDict({\n",
    "                name: ImportancePredictor(self.imp_hidden_size, self.C, device=device) for name in [\"embed\", \"unembed\"]                    \n",
    "            })]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, masks=None, return_activs_weights=False):\n",
    "        # Create the masks object so that dict keys never break (simplifies code)\n",
    "        if masks is None:\n",
    "            layer_masks = [\n",
    "                { name: None for name in (\"in\", \"out\") }\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "            end_masks = { name: None for name in (\"embed\", \"unembed\") }\n",
    "            masks = layer_masks + [end_masks]\n",
    "\n",
    "        \n",
    "        activations = []\n",
    "\n",
    "        # regular forward pass\n",
    "        x, embed_activs = self.embed(x, masks[-1][\"embed\"])\n",
    "        for l in range(self.num_layers):\n",
    "            x, layer_activations = self.layers[l](x, masks=masks[l])\n",
    "            activations.append(layer_activations)\n",
    "            \n",
    "        x, unembed_activs = self.unembed(x, masks[-1][\"unembed\"])\n",
    "        activations.append({\"embed\": embed_activs, \"unembed\": unembed_activs})             \n",
    "        \n",
    "        \n",
    "        if not return_activs_weights:\n",
    "            return x\n",
    "        else:\n",
    "            weights = []\n",
    "\n",
    "            for l in range(self.num_layers):\n",
    "                weights.append(self.layers[l].return_weights_layer())\n",
    "            weights.append({\"embed\": self.embed.return_weights(), \"unembed\": self.unembed.return_weights()})\n",
    "            \n",
    "            return x, activations, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8539ebae-274f-4d8c-83a5-c57ce0cbfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the hard sigmoid activation function as described in the paper:\n",
    "        σ_H(x) = 0 if x <= 0\n",
    "               = x if 0 < x < 1\n",
    "               = 1 if x >= 1\n",
    "    This is equivalent to: torch.clamp(x, min=0.0, max=1.0)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Clamp values between 0 and 1\n",
    "        return torch.clamp(x, min=0.0, max=1.0)\n",
    "\n",
    "\n",
    "class LowerLeakyHardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Lower-leaky hard sigmoid: σH,lower(x)\n",
    "    - 0.01*x if x <= 0 (leaky below 0)\n",
    "    - x if 0 <= x <= 1 (linear in middle)  \n",
    "    - 1 if x >= 1 (saturated above 1)\n",
    "    \n",
    "    Used for forward pass masks in stochastic reconstruction losses.\n",
    "    \"\"\"\n",
    "    def __init__(self, leak_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.leak_slope = leak_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(\n",
    "            x <= 0, \n",
    "            self.leak_slope * x,\n",
    "            torch.where(\n",
    "                x >= 1,\n",
    "                torch.ones_like(x),\n",
    "                x\n",
    "            )\n",
    "        )\n",
    "\n",
    "class UpperLeakyHardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Upper-leaky hard sigmoid: σH,upper(x)  \n",
    "    - 0 if x <= 0 (hard cutoff below 0)\n",
    "    - x if 0 <= x <= 1 (linear in middle)\n",
    "    - 1 + 0.01*(x-1) if x >= 1 (leaky above 1)\n",
    "    \n",
    "    Used for importance loss computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, leak_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.leak_slope = leak_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(\n",
    "            x <= 0,\n",
    "            torch.zeros_like(x), \n",
    "            torch.where(\n",
    "                x >= 1,\n",
    "                1 + self.leak_slope * (x - 1),\n",
    "                x\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf25bf6a-c788-4186-8190-b7d136c30949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spd(spd_model, dataloader, train_config, num_epochs=1):\n",
    "\n",
    "    # SPD model is a model\n",
    "    spd_model.train()\n",
    "    print(f\"Training on device {spd_model.device}\")\n",
    "    optimizer = torch.optim.AdamW(spd_model.parameters(), lr = train_config[\"lr\"])\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, \n",
    "        step_size=train_config[\"lr_step_size\"],   # e.g. every 4 epochs\n",
    "        gamma=train_config[\"lr_gamma\"]      # multiply LR by 0.5 each time\n",
    "    )\n",
    "    \n",
    "    # P = sum(p.numel() for p in spd_model.target_model.parameters()) [moved into model]\n",
    "    upper_leaky_sigmoid = UpperLeakyHardSigmoid()\n",
    "    lower_leaky_sigmoid = LowerLeakyHardSigmoid()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, \n",
    "        step_size=train_config[\"lr_step_size\"],\n",
    "        gamma=train_config[\"lr_gamma\"]\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_l_stoch_rec, total_l_stoch_rec_l, total_l_imp, total_l_faith = 0.0, 0.0, 0.0, 0.0\n",
    "        epoch_lr = f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        print(f\"Starting epoch {epoch+1}, lr = {epoch_lr}\")\n",
    "\n",
    "        with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as t:\n",
    "            for batch_idx, (x,y) in enumerate(t):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                N = x.shape[0]     # x is shape N by in_size\n",
    "                S, C, P, L = spd_model.hypers[\"num_mask_samples\"], spd_model.C, spd_model.P, spd_model.num_layers\n",
    "                target_model = spd_model.target_model\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # ====== TARGET MODEL OUTPUT ======\n",
    "                with torch.no_grad(): \n",
    "                    target_out = spd_model.target_model(x)\n",
    "\n",
    "                # ====== FAITHFULNESS LOSS ======\n",
    "                spd_output, spd_activations, spd_weights = spd_model(x, return_activs_weights = True)\n",
    "                squared_error = 0\n",
    "\n",
    "                for l in range(L):\n",
    "                    in_diff = target_model.W_in[l] - spd_weights[l][\"in\"]\n",
    "                    out_diff = target_model.W_out[l] - spd_weights[l][\"out\"]\n",
    "                    squared_error_layer = torch.linalg.matrix_norm(in_diff)**2 + torch.linalg.matrix_norm(out_diff)**2 \n",
    "                    squared_error = squared_error + squared_error_layer\n",
    "\n",
    "                embed_diff = target_model.W_embed - spd_weights[-1][\"embed\"]\n",
    "                unembed_diff = target_model.W_unembed - spd_weights[-1][\"unembed\"]\n",
    "                squared_error_embed = torch.linalg.matrix_norm(embed_diff)**2 + torch.linalg.matrix_norm(unembed_diff)**2 \n",
    "                squared_error = squared_error + squared_error_embed\n",
    "\n",
    "                l_faithfulness = squared_error/P # \"Mean Squared Error\" across parameters\n",
    "                \n",
    "\n",
    "                # ===== IMPORTANCE MINIMALITY LOSS ======\n",
    "                l_importance_minimality = 0.0\n",
    "\n",
    "                pred_importances = []\n",
    "\n",
    "                imp_pred_embed = spd_model.imp_pred_ers[-1][\"embed\"](spd_activations[-1][\"embed\"])\n",
    "                imp_pred_unembed = spd_model.imp_pred_ers[-1][\"unembed\"](spd_activations[-1][\"unembed\"])\n",
    "                l_importance_minimality = l_importance_minimality + (upper_leaky_sigmoid(imp_pred_embed)** spd_model.hypers[\"causal_imp_min\"]).sum() + (upper_leaky_sigmoid(imp_pred_unembed) ** spd_model.hypers[\"causal_imp_min\"]).sum()\n",
    "                for l in range(L):\n",
    "                    imp_pred_in = spd_model.imp_pred_ers[l][\"in\"](spd_activations[l][\"in\"])\n",
    "                    imp_pred_out = spd_model.imp_pred_ers[l][\"out\"](spd_activations[l][\"out\"])\n",
    "                    pred_importances.append({\"in\": imp_pred_in, \"out\": imp_pred_out})\n",
    "                    l_importance_minimality = l_importance_minimality + (upper_leaky_sigmoid(imp_pred_in) ** spd_model.hypers[\"causal_imp_min\"]).sum()  + (upper_leaky_sigmoid(imp_pred_out)** spd_model.hypers[\"causal_imp_min\"]).sum() \n",
    "                pred_importances.append({\"embed\": imp_pred_embed, \"unembed\": imp_pred_unembed})\n",
    "\n",
    "                l_importance_minimality /= N\n",
    "\n",
    "\n",
    "                # ===== STOCHASTIC RECONSTRUCTION LOSS ======\n",
    "                l_stochastic_recon = 0.0\n",
    "                l_stochastic_recon_layerwise = 0.0\n",
    "                R = torch.rand((S, N, L+1, 2, C), device=device)\n",
    "\n",
    "                # we have activs in shape (N,C). Stack and unsqueeze(0) to (1,2,N,C) and cat along dim 1 to get (L+1,2,N,C)\n",
    "                stacked_imps = torch.cat([torch.stack((pred_importances[l][\"in\"], pred_importances[l][\"out\"])).reshape(1,2,N,C) for l in range(L)] + [torch.stack((pred_importances[-1][\"embed\"],pred_importances[-1][\"unembed\"])).reshape(1,2,N,C)])\n",
    "                # reshape (L+1, 2, N, C) -> (N, L+1, 2, C)\n",
    "                stacked_imps = torch.movedim(stacked_imps, 2, 0)\n",
    "                # Apply sigmoid and then reshape to (1, N, L+1, 2, C)\n",
    "                G = lower_leaky_sigmoid(stacked_imps).unsqueeze(0)\n",
    "                masks = G + (1-G)*R # shape (S, N, L+1, 2, C)\n",
    "                masks = masks.reshape(S*N, L+1, 2, C)\n",
    "\n",
    "                # Move masks back into layerwise structure\n",
    "                layer_masks = [\n",
    "                    { name: masks[:,l,idx,:] for name, idx in [(\"in\",0), (\"out\",1)] } for l in range(L)\n",
    "                ]\n",
    "                end_masks = { name: masks[:,l+1,idx,:] for name, idx in [(\"embed\", 0), (\"unembed\", 1)] }\n",
    "                masks_dictified = layer_masks + [end_masks]\n",
    "\n",
    "                #Tile inputs (1, N, embed_size) S times to have N_new = S * N -> (N * S, embed_size)\n",
    "                x_repeated = x.clone().unsqueeze(0).expand(S, -1, -1).reshape(S * N, -1)\n",
    "\n",
    "                # Run regular masked loss and update stochastic recon\n",
    "                masked_out = spd_model(x_repeated, masks=masks_dictified)\n",
    "                target_out_tiled = target_out.clone().unsqueeze(0).expand(S, -1, -1).reshape(S*N, -1)\n",
    "                l_stochastic_recon = l_stochastic_recon + torch.linalg.matrix_norm(target_out_tiled-masked_out)**2\n",
    "                l_stochastic_recon /= S\n",
    "\n",
    "                # ===== STOCHASTIC RECONSTRUCTION LAYERWISE LOSS ======\n",
    "                # Create Layerwise Mask constructor\n",
    "                layer_masks_none = [\n",
    "                    { name: None for name in (\"in\", \"out\") }\n",
    "                    for _ in range(L)\n",
    "                ]\n",
    "                end_masks_none = { name: None for name in (\"embed\", \"unembed\") }\n",
    "                layer_mask_constructor = layer_masks_none + [end_masks_none]\n",
    "\n",
    "                # Embed and unembed outs\n",
    "                embed_mask = layer_mask_constructor.copy()\n",
    "                embed_mask[-1][\"embed\"] = masks_dictified[-1][\"embed\"]\n",
    "                embed_out=spd_model(x_repeated, embed_mask)\n",
    "\n",
    "                unembed_mask = layer_mask_constructor.copy()\n",
    "                unembed_mask[-1][\"unembed\"] = masks_dictified[-1][\"unembed\"]\n",
    "                unembed_out=spd_model(x_repeated,unembed_mask)\n",
    "                l_stochastic_recon_layerwise = l_stochastic_recon_layerwise + torch.linalg.matrix_norm(target_out_tiled-embed_out)**2 + torch.linalg.matrix_norm(target_out_tiled-unembed_out)**2\n",
    "\n",
    "                for l in range(l): \n",
    "                    for name in [\"in\", \"out\"]:\n",
    "                        layer_mask = layer_mask_constructor.copy()\n",
    "                        layer_mask[l][name] = masks_dictified[l][name]\n",
    "                        layer_out = spd_model(x_repeated,layer_mask)\n",
    "                        l_stochastic_recon_layerwise = l_stochastic_recon_layerwise + torch.linalg.matrix_norm(target_out_tiled-embed_out)**2\n",
    "                        \n",
    "                l_stochastic_recon_layerwise /= (S * spd_model.num_matrices)\n",
    "                                \n",
    "                beta1, beta2, beta3 = spd_model.hypers[\"beta_1\"], spd_model.hypers[\"beta_2\"], spd_model.hypers[\"beta_3\"]\n",
    "\n",
    "                # Loss computations\n",
    "                loss = l_faithfulness + beta1*l_stochastic_recon + beta2*l_stochastic_recon_layerwise + beta3*l_importance_minimality\n",
    "                print(type(float(epoch_lr)))\n",
    "                run.log({\"lr\": float(epoch_lr), \"loss\": loss, \"l_faithfulness\": l_faithfulness, \"l_stochastic_recon\": l_stochastic_recon, \"l_stochastic_recon_layerwise\": l_stochastic_recon_layerwise, \"l_importance_minimality\": l_importance_minimality})\n",
    "                loss.backward()\n",
    "                optimizer.step() \n",
    "                t.set_postfix(loss=loss.item())\n",
    "            print(type(float(epoch_lr)))\n",
    "            print(f\"(Last batch) Faithfulness: {l_faithfulness}, Stoch Rec: {l_stochastic_recon}, Stoch Rec Layerwise: {l_stochastic_recon_layerwise}, Importance Min: {l_importance_minimality}\")\n",
    "        scheduler.step()\n",
    "        \n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a42a9-0311-4293-9a83-8b3093cb0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    run = wandb.init(\n",
    "        entity=\"lgngrvs-independent\", \n",
    "        project=\"spd\",\n",
    "        config = CONFIG | TRAIN_CONFIG # Join dicts\n",
    "    )\n",
    "\n",
    "    device = DEVICE # specified at beginning of file\n",
    "    config = CONFIG\n",
    "    train_config = TRAIN_CONFIG\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    spd_model = SPDModelMLP(toy_model, config, device)\n",
    "    train_spd(spd_model, dataloader, train_config, num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99df0a71-cd6e-4d7d-99cf-70b75fd367db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPD Model: \n",
      " tensor([[ 0.0329, -0.0324,  0.0177,  ..., -0.0074,  0.0089, -0.0027],\n",
      "        [ 0.0168, -0.0347,  0.0291,  ..., -0.0363,  0.0379, -0.0433],\n",
      "        [-0.0384,  0.0381,  0.0118,  ..., -0.0195, -0.0077,  0.0173],\n",
      "        ...,\n",
      "        [-0.0029, -0.0222,  0.0157,  ...,  0.0014, -0.0002, -0.0081],\n",
      "        [-0.0235, -0.0095, -0.0057,  ..., -0.0379, -0.0104,  0.0051],\n",
      "        [ 0.0114,  0.0214,  0.0185,  ...,  0.0210,  0.0139, -0.0212]],\n",
      "       device='mps:0', grad_fn=<MmBackward0>)\n",
      "Target Model: \n",
      " tensor([[ 1.6213e-02, -1.4922e-02,  1.1248e-02,  ..., -3.1487e-03,\n",
      "         -3.6299e-04, -8.5673e-03],\n",
      "        [ 9.6031e-04, -1.3385e-02,  2.1028e-02,  ..., -1.5281e-02,\n",
      "          1.7071e-02, -1.4203e-02],\n",
      "        [-1.1780e-04,  7.9431e-03,  3.6117e-03,  ..., -9.5580e-03,\n",
      "          1.9460e-03,  1.9929e-02],\n",
      "        ...,\n",
      "        [ 9.5115e-03, -8.3585e-03, -6.8799e-05,  ...,  8.7297e-03,\n",
      "          7.1573e-03,  4.3572e-03],\n",
      "        [-1.2532e-02, -4.4453e-03, -1.6739e-02,  ..., -1.2532e-02,\n",
      "          4.9728e-03, -8.7136e-03],\n",
      "        [-1.0624e-03,  9.0949e-04,  5.3736e-03,  ...,  2.1696e-02,\n",
      "          9.0125e-03, -6.9974e-03]], device='mps:0', grad_fn=<ViewBackward0>)\n",
      "tensor([[-0.0040, -0.0084, -0.0168,  ..., -0.0151, -0.0097, -0.0124],\n",
      "        [-0.0044, -0.0061, -0.0194,  ..., -0.0135, -0.0059, -0.0212],\n",
      "        [-0.0072, -0.0083, -0.0212,  ..., -0.0149, -0.0073, -0.0137],\n",
      "        ...,\n",
      "        [-0.0055, -0.0059, -0.0158,  ..., -0.0078, -0.0134, -0.0175],\n",
      "        [-0.0069, -0.0087, -0.0119,  ..., -0.0151, -0.0058, -0.0355],\n",
      "        [-0.0086, -0.0111, -0.0025,  ..., -0.0046, -0.0037, -0.0082]],\n",
      "       device='mps:0', grad_fn=<AddBackward0>) tensor([[-0.0051, -0.0025, -0.0078,  ..., -0.0020, -0.0080, -0.0030],\n",
      "        [-0.0086, -0.0046, -0.0023,  ..., -0.0024, -0.0054, -0.0046],\n",
      "        [-0.0113, -0.0057, -0.0029,  ..., -0.0020, -0.0139, -0.0020],\n",
      "        ...,\n",
      "        [-0.0131, -0.0059, -0.0055,  ..., -0.0014, -0.0135, -0.0035],\n",
      "        [-0.0158, -0.0042, -0.0012,  ..., -0.0037, -0.0166, -0.0061],\n",
      "        [-0.0174, -0.0044, -0.0098,  ..., -0.0014, -0.0115, -0.0021]],\n",
      "       device='mps:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Setup evaluation dataset and dataloader\n",
    "eval_dataset = SparseAutoencoderDataset(in_dim=100, n_samples=100, sparsity=0.9, device=\"mps\")\n",
    "dataloader = DataLoader(eval_dataset, batch_size=128, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "# Evaluate both models\n",
    "out_eval = spd_model(batch[0])\n",
    "out_eval_target = spd_model.target_model(batch[0])\n",
    "print(\"SPD Model: \\n\", out_eval)\n",
    "print(\"Target Model: \\n\", out_eval_target)\n",
    "\n",
    "def check_masks(x, spd_model):\n",
    "    pred_importances = []\n",
    "    # Use the correct keyword argument for the refactored model\n",
    "    spd_output, spd_activations, spd_weights = spd_model(x, return_activs_weights=True)\n",
    "    \n",
    "    # Use the class-based importance predictors\n",
    "    # For embed/unembed (final layer)\n",
    "    imp_pred_embed = spd_model.imp_pred_ers[-1][\"embed\"](spd_activations[-1][\"embed\"])\n",
    "    imp_pred_unembed = spd_model.imp_pred_ers[-1][\"unembed\"](spd_activations[-1][\"unembed\"])\n",
    "    \n",
    "    # For each layer\n",
    "    for l in range(spd_model.num_layers):\n",
    "        imp_pred_in = spd_model.imp_pred_ers[l][\"in\"](spd_activations[l][\"in\"])\n",
    "        imp_pred_out = spd_model.imp_pred_ers[l][\"out\"](spd_activations[l][\"out\"])\n",
    "        pred_importances.append({\"in\": imp_pred_in, \"out\": imp_pred_out})\n",
    "    \n",
    "    # Add embed/unembed at the end\n",
    "    pred_importances.append({\"embed\": imp_pred_embed, \"unembed\": imp_pred_unembed})\n",
    "    return pred_importances\n",
    "\n",
    "# Run the mask checker\n",
    "importances = check_masks(batch[0], spd_model)\n",
    "print(importances[0][\"in\"], importances[0][\"out\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212615f-d885-474e-8990-c07db593cbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-replication",
   "language": "python",
   "name": "interp-replication"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
