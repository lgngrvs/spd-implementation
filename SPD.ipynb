{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5b19f-26f8-4a66-85ca-5f81c949767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the torch stuff, as well as the ViT stuff and such\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c791fb62-d6be-4926-b345-c518a5239fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import einops\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ea7aa-613d-4354-95c0-44fbdcc7849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your original toy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48df6266-0231-4c6d-a813-19f4bf12553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SPDHookedTransformer Class\n",
    "# that extends the hookedTransformer\n",
    "# which inherits all the previous class stuff\n",
    "# but also contains the SPD training algo?\n",
    "\n",
    "# how is everything actually structured? \n",
    "\"\"\"\n",
    "Maybe I should start with implementing it on a simple MLP setup. \n",
    "\n",
    "Orig network:\n",
    "- 1 layer MLP 5-2-5 (defined as usual with torch.Sequential presumably)\n",
    "\n",
    "SPD network:\n",
    "let's give it 10 subcomponents per layer\n",
    "then it's just like 10 matmuls? \n",
    "maybe i should define it like ant did in the toy models of superposition paper\n",
    "stick them all into a trenchcoat\n",
    "this might be easier once I have defined the toy modle\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50be6f25-5489-4445-82c0-914779cc7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_layers\": 2,\n",
    "    \"in_size\": 10,\n",
    "    \"hidden_size\": 5,\n",
    "    \"subcomponents_per_layer\": 5, \n",
    "    \"beta_1\": 1, \n",
    "    \"beta_2\": 1, \n",
    "    \"beta_3\": 1, \n",
    "    \"causal_imp_min\": 1, \n",
    "    \"num_samples\": 10,\n",
    "    \"importance_mlp_size\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a426a65-b7dc-4e68-9e35-da69dbdf7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyResidMLP(nn.Module):\n",
    "    def __init__(self, config, device=\"mps\"):\n",
    "        super().__init__()\n",
    "        # Initialize Weights for the\n",
    "        self.num_layers, self.in_size, self.hidden_size = config[\"num_layers\"], config[\"in_size\"], config[\"hidden_size\"]\n",
    "        self.device = device\n",
    "        self.W_in = nn.ParameterList([torch.empty((in_size, hidden_size), device=device) for i in range(num_layers)])\n",
    "        self.W_out = nn.ParameterList([torch.empty((hidden_size, in_size), device=device) for i in range(num_layers)])\n",
    "        self.b = nn.ParameterList([torch.zeros((hidden_size,), device=device) for i in range(num_layers)])\n",
    "\n",
    "        for param in self.W_in + self.W_out: \n",
    "            nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        # apply matmul lol\n",
    "        N, D = x.shape\n",
    "        assert D == self.in_size, f\"Input shape does not match model's accepted size {self.in_size}\"\n",
    "        # something to ensure shape?\n",
    "        \n",
    "        x_resid = x\n",
    "        for i in range(self.num_layers):\n",
    "            hidden = F.relu(torch.einsum(\"nd,dh -> nh\", x_resid, self.W_in[i]))\n",
    "            layer_out = torch.einsum(\"nh,hd -> nd\", hidden, self.W_out[i])\n",
    "            x_resid += layer_out\n",
    "        # am I supposed to have a embed and out?\n",
    "        return x_resid\n",
    "\n",
    "\n",
    "def toy_train(model, lr, num_steps): \n",
    "    # init AdamW optimizer on model\n",
    "    # wrap train function for tqdm\n",
    "    # for step in num_step:\n",
    "    # generate batch for the step\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50310679-3db3-4b7d-8a8b-24c32dc176e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPDModelMLP(nn.Module): \n",
    "    # fix the betas stuff below that's basically a type hint\n",
    "    def __init__(self, target_model, config, device=\"mps\"): \n",
    "        self.device = device\n",
    "        self.target_model = target_model\n",
    "        self.num_layers, self.in_size, self.hidden_size, self.imp_hidden_size = config[\"num_layers\"], config[\"in_size\"], config[\"hidden_size\"], config[\"importance_mlp_size\"]\n",
    "        assert self.device == target_model.device, \"Models not on same device\"\n",
    "        self.C = config[\"subcomponents_per_layer\"]\n",
    "        \n",
    "        # Subcomponent vectors, each of shape C by in_size; to be used\n",
    "        # with outer product to create our low-rank subcomponent matrices\n",
    "        self.V_in = nn.ParameterList([torch.empty((self.C, in_size,), device=device) for i in range(num_layers)])\n",
    "        self.U_in = nn.ParameterList([torch.empty((self.C, hidden_size,), device=device) for i in range(num_layers)]) \n",
    "        self.V_out = nn.ParameterList([torch.empty((self.C, hidden_size,), device=device) for i in range(num_layers)])\n",
    "        self.U_out = nn.ParameterList([torch.empty((self.C, in_size,), device=device) for i in range(num_layers)])\n",
    "        \n",
    "        # idk what you do with the biases lol\n",
    "        self.b = nn.ParameterList([torch.zeros((hidden_size,), device=device) for i in range(num_layers)])\n",
    "        \n",
    "        # imp_W_in and out etc are the weights for the importance predictor\n",
    "        # should be C networks per layer, mapping 1 -> C -> 1\n",
    "        self.imp_W_in = nn.ParameterList([torch.empty(C, 1, self.imp_hidden_size) for i in range(num_layers)])\n",
    "        self.imp_W_out = nn.ParameterList([torch.empty(C, self.imp_hidden_size, 1) for i in range(num_layers)])\n",
    "        self.imp_b_in = nn.ParameterList([torch.empty(C, self.imp_hidden_size) for i in range(num_layers)])\n",
    "        self.imp_b_out = nn.ParameterList([torch.empty(C, 1) for i in range(num_layers)])\n",
    "        \n",
    "        \n",
    "        # self.pred_weights = torch.empty((num_layers, self.C), device=device)\n",
    "\n",
    "        for param in self.V_in + self.U_in + self.V_out + self.U_out + self.b: \n",
    "            # using xavier anyway -- note that the variance etc is\n",
    "            # changed because we take the outer product in the \n",
    "            # forward pass\n",
    "            nn.init.xavier_normal_(param)\n",
    "\n",
    "        \n",
    "    def forward_with_activations(self, x): # Regular run. Unclear whether I should have masking when I do a regular forward pass.\n",
    "        activations = []\n",
    "        weight_matrices = []\n",
    "        x_resid = x\n",
    "        \n",
    "        for i in self.num_layers:\n",
    "            # Use outer product to create weights for the layer, then sum all the subcomponents\n",
    "            W_in = torch.einsum(\"ci,ch-> cih\", self.V_in[i], self.U_in[i]).sum(dim=0) # shape i h\n",
    "            W_out = torch.einsum(\"ch,ci-> chi\", self.V_out[i], self.U_out[i]).sum(dim=0) # shape h i\n",
    "            weight_matrices.append({\"in\": W_in, \"out\", W_out})\n",
    "            \n",
    "            layer_activations = torch.einsum(\"di,ih -> dh\", x_resid, W_in) + self.b[i]\n",
    "            activations.append(layer_activations) \n",
    "            # THIS IS WRONG, NOT WHAT I WANT\n",
    "            # NEED TO MAKE ACTIVATIONS BE THE V FOR EACH INDIV COMPONENT. REORDER THE \n",
    "            # OPERATIONS SO THAT THE COMPONENTS ACTIV FIRST, THEN SUM ALONG C? \n",
    "            # NO :( I USE THAT PRECOMPUTED WEIGHT FOR LATER. I GUESS WE CAN JUST \n",
    "            # RUN IT TWICE. SEEMS LIKE THIS MIGHT JUST BE INEFFICIENT AF \n",
    "            # OR MORE LIKELY SKILL ISSUE\n",
    "            \n",
    "            layer_out = torch.einsum(\"dh,hi->di\", F.relu(layer_activations), W_out)\n",
    "            \n",
    "            x_resid += layer_out\n",
    "        return x_resid, activations, weight_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731eff5-4248-4332-85c5-ca3afc41f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(config): \n",
    "    # return shape N, config.in_size \n",
    "    pass\n",
    "\n",
    "def train_SPD(spd_model): # could also implement this by passing in the original model\n",
    "    # generate batch somehow\n",
    "    x = generate_batch # TODO (above). possibly put in the model? idk if needed.\n",
    "    # x is shape N by in_size\n",
    "    target_model = spd_model.target_model\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        target_out = spd_model.target_model(x)\n",
    "\n",
    "    # MSE Loss\n",
    "    spd_output, spd_activations spd_weights = model.forward_with_activations(x)\n",
    "    squared_error = 0\n",
    "    for i in range(num_layers):\n",
    "        in_diff = target_model.W_in[i] - spd_weights[i][\"in\"]\n",
    "        out_diff = target_model.W_out[i] - spd_weights[i][\"out\"]\n",
    "        \n",
    "        # torch.linalg.matrix_norm defaults to the frobenius norm\n",
    "        # this takes the frobenius norm of the diff and then squares it\n",
    "        squared_error_layer = torch.linalg.matrix_norm(in_diff) ** 2 + torch.linalg.matrix_norm(out_diff) ** 2 \n",
    "        squared_error += squared_error_layer\n",
    "        \n",
    "    mean_squared_error = squared_error/num_layers\n",
    "    l_faithfulness = mean_squared_error\n",
    "\n",
    "    # Predict importance\n",
    "\n",
    "    for i in range(num_layers): \n",
    "        # confused about this -- should the activations \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351b0fc-8336-4d5d-86c5-70c6193844d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward pass is like\n",
    "0. generate or sample datafs\n",
    "1. run the regular model\n",
    "2. run SPD model\n",
    "3. faithfulness loss (check that the SPD model weights sum to the original model)\n",
    "4. Get 'intermediate activations' and then get MLP predictions for each layer\n",
    "5. compute importance-minimality loss\n",
    "6. Sample Rs and compute masked weights\n",
    "7. run model with random masking, with \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-replication",
   "language": "python",
   "name": "interp-replication"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
