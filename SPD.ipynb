{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374687f7-c6bd-462e-92cb-95a74322304b",
   "metadata": {},
   "source": [
    "# Stochastic Parameter Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4be581-416b-4231-b780-c5cabb7142f6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5b19f-26f8-4a66-85ca-5f81c949767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the torch stuff, as well as the ViT stuff and such\n",
    "# import plotly.express as px\n",
    "# import torch\n",
    "# from jaxtyping import Int, Float\n",
    "# from typing import List, Optional, Tuple\n",
    "# from tqdm import tqdm\n",
    "# from transformer_lens.hook_points import HookPoint\n",
    "# from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "# import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c791fb62-d6be-4926-b345-c518a5239fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import einops\n",
    "import typing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd14d16-8e3e-4efe-9d5f-7f29610d6b83",
   "metadata": {},
   "source": [
    "## Toy Model MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48df6266-0231-4c6d-a813-19f4bf12553b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create a SPDHookedTransformer Class\n",
    "# that extends the hookedTransformer\n",
    "# which inherits all the previous class stuff\n",
    "# but also contains the SPD training algo?\n",
    "\n",
    "# how is everything actually structured? \n",
    "\"\"\"\n",
    "Maybe I should start with implementing it on a simple MLP setup. \n",
    "\n",
    "Orig network:\n",
    "- 1 layer MLP 5-2-5 (defined as usual with torch.Sequential presumably)\n",
    "\n",
    "SPD network:\n",
    "let's give it 10 subcomponents per layer\n",
    "then it's just like 10 matmuls? \n",
    "maybe i should define it like ant did in the toy models of superposition paper\n",
    "stick them all into a trenchcoat\n",
    "this might be easier once I have defined the toy modle\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "50be6f25-5489-4445-82c0-914779cc7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"config = {\n",
    "    \"num_layers\": 1,\n",
    "    \"pre_embed_size\": 100,\n",
    "    \"in_size\": 1000,\n",
    "    \"hidden_size\": 50,\n",
    "    \"subcomponents_per_layer\": 10, \n",
    "    \"beta_1\": 1, \n",
    "    \"beta_2\": 1, \n",
    "    \"beta_3\": 1, \n",
    "    \"causal_imp_min\": 1, \n",
    "    \"num_mask_samples\": 20,\n",
    "    \"importance_mlp_size\": 10,\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5a426a65-b7dc-4e68-9e35-da69dbdf7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyResidMLP(nn.Module):\n",
    "    def __init__(self, config, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        # Initialize Weights for the\n",
    "        self.num_layers, self.pre_embed_size, self.in_size, self.hidden_size = config[\"num_layers\"], config[\"pre_embed_size\"], config[\"in_size\"], config[\"hidden_size\"]\n",
    "        self.device = device\n",
    "        self.W_embed = nn.Parameter(torch.empty((self.pre_embed_size, self.in_size)))\n",
    "        self.W_unembed = nn.Parameter(torch.empty((self.in_size, self.pre_embed_size)))\n",
    "        self.W_in = nn.ParameterList([torch.empty((self.in_size, self.hidden_size), device=device) for i in range(self.num_layers)])\n",
    "        self.W_out = nn.ParameterList([torch.empty((self.hidden_size, self.in_size), device=device) for i in range(self.num_layers)])\n",
    "        self.b = nn.ParameterList([torch.zeros((self.hidden_size,), device=device) for i in range(self.num_layers)])\n",
    "\n",
    "        for param in [self.W_embed, self.W_unembed] + list(self.W_in) + list(self.W_out): \n",
    "            nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        \n",
    "        assert x.shape[1] == self.pre_embed_size, f\"Input shape {x.shape[0]} does not match model's accepted size {self.pre_embed_size}\"\n",
    "        # embed \n",
    "        x_resid = torch.einsum(\"np,pi->ni\", x.clone(), self.W_embed)\n",
    "        N, D = x_resid.shape\n",
    "\n",
    "        \n",
    "        for l in range(self.num_layers):\n",
    "            hidden = F.relu(torch.einsum(\"nd,dh -> nh\", x_resid, self.W_in[l]) + self.b[l])\n",
    "            layer_out = torch.einsum(\"nh,hd -> nd\", hidden, self.W_out[l])\n",
    "            x_resid = x_resid + layer_out\n",
    "        # am I supposed to have a embed and out?\n",
    "        x_out = torch.einsum(\"ni,ip->np\", x_resid, self.W_unembed) \n",
    "        return x_out\n",
    "\n",
    "\n",
    "# Generated by LLM bc I am lazy and this is not important to me learning stuff atm\n",
    "class SparseAutoencoderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for learning to reconstruct sparse inputs.\n",
    "    Each item is (input, target) where target is the input itself (or ReLU of input).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=100, n_samples=10000, sparsity=0.9, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.n_samples = n_samples\n",
    "        self.device = device\n",
    "\n",
    "        # Pre-generate all samples\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Sparse input: each entry is -1, 0, or 1, with sparsity\n",
    "            x = np.random.choice([0, -1, 1], size=(in_dim,), p=[1-sparsity, sparsity/2, sparsity/2])\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "            target = F.relu(x)\n",
    "            \n",
    "            self.inputs.append(x)\n",
    "            self.targets.append(target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "def train_toy_resid_mlp(\n",
    "    model,\n",
    "    dataloader,\n",
    "    lr=1e-3,\n",
    "    num_epochs=10,\n",
    "    device=\"cpu\",\n",
    "    print_every=1\n",
    "):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for x, y in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}: avg MSE loss = {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d67d9bbc-74ee-4eca-8a38-90c245095172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 104.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg MSE loss = 1955126.785463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 113.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg MSE loss = 734.124697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 115.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: avg MSE loss = 126.455034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 115.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg MSE loss = 57.345500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 113.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: avg MSE loss = 31.339699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 115.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: avg MSE loss = 18.719937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 113.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: avg MSE loss = 11.961505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 102.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: avg MSE loss = 7.949900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 107.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: avg MSE loss = 5.459462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 113.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: avg MSE loss = 3.857026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 112.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: avg MSE loss = 2.798680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 111.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: avg MSE loss = 2.082040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 114.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: avg MSE loss = 1.585487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 114.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: avg MSE loss = 1.236326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 114.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: avg MSE loss = 0.989475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 105.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: avg MSE loss = 0.812974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 114.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: avg MSE loss = 0.685128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 115.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: avg MSE loss = 0.592591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 113.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: avg MSE loss = 0.525682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 114.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: avg MSE loss = 0.477137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## LLM-Generated Usage Example\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Config\n",
    "    # device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "    # Dataset and DataLoader\n",
    "    # Dataset and DataLoader\n",
    "    dataset = SparseAutoencoderDataset(\n",
    "        in_dim=100,\n",
    "        n_samples=10000,\n",
    "        sparsity=0.9,\n",
    "        device=device,\n",
    "    )\n",
    "    print(device)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    # Model\n",
    "    toy_model = ToyResidMLP(config, device=device)\n",
    "    # Train\n",
    "    train_toy_resid_mlp(toy_model, dataloader, lr=8e-2, num_epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c72b4-6661-4d00-b27d-cee3f7ac01ac",
   "metadata": {},
   "source": [
    "## SPD Model and Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9bda061-7487-44ce-9201-dc1f6a34b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000, 0.3000, 0.7000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# generated by LLM\n",
    "\n",
    "class HardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the hard sigmoid activation function as described in the paper:\n",
    "        σ_H(x) = 0 if x <= 0\n",
    "               = x if 0 < x < 1\n",
    "               = 1 if x >= 1\n",
    "    This is equivalent to: torch.clamp(x, min=0.0, max=1.0)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Clamp values between 0 and 1\n",
    "        return torch.clamp(x, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "50310679-3db3-4b7d-8a8b-24c32dc176e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices:  cpu cpu\n",
      "Training on device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:40<00:00,  1.93it/s, loss=1.12e+11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss = 32270779502.052044\n",
      "12178.3916015625 261927024.0 0.0 0.08737314492464066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  41%|██████████████████████████████████▊                                                   | 32/79 [00:16<00:24,  1.91it/s, loss=4.38e+12]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Subcomponent activations not calculated correctly, max difference is 9.9375; tensor([[1014606.5000, -265019.7188,  666373.5000,  ..., -427995.0312,\n          218633.4062, 1629082.1250],\n        [ -25435.8477,    7606.3452,   56588.8906,  ...,  -40041.9414,\n           33080.8945,  127081.9062],\n        [   9664.1396,  291722.1250, -148703.9219,  ...,  393290.3750,\n         -896824.0000, -758538.4375],\n        ...,\n        [-160292.5625, -126715.6875,  147662.5938,  ..., -216781.6094,\n          412628.8750,  442505.0625],\n        [-201789.5625,   40519.5156,  -18451.2734,  ...,  -56442.4766,\n          120751.6641,  -22576.2910],\n        [-158896.9219,  -89091.0234,  -94191.8281,  ...,  -27028.8262,\n          335013.8125,  -67209.3047]], grad_fn=<AddBackward0>)tensor([[1014615.6875, -265011.1562,  666366.3125,  ..., -427994.5000,\n          218640.9844, 1629091.2500],\n        [ -25427.0684,    7614.9150,   56581.7930,  ...,  -40041.4453,\n           33088.3633,  127091.1953],\n        [   9672.9824,  291730.6875, -148710.9219,  ...,  393291.0000,\n         -896816.5000, -758529.6875],\n        ...,\n        [-160283.7188, -126707.2266,  147655.5469,  ..., -216781.1094,\n          412636.5000,  442514.5938],\n        [-201780.8750,   40528.0820,  -18458.3379,  ...,  -56442.0156,\n          120759.2734,  -22567.0059],\n        [-158888.2188,  -89082.5234,  -94198.9766,  ...,  -27028.3184,\n          335021.4375,  -67200.0234]], grad_fn=<ViewBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[261]\u001b[39m\u001b[32m, line 183\u001b[39m\n\u001b[32m    181\u001b[39m spd_model = SPDModelMLP(toy_model, config, device=device)\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mtrain_SPD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspd_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[258]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtrain_SPD\u001b[39m\u001b[34m(spd_model, dataloader, lr, num_epochs)\u001b[39m\n\u001b[32m     25\u001b[39m     target_out = spd_model.target_model(x)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# FAITHFULNESS LOSS\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m spd_output, spd_activations, spd_weights = \u001b[43mspd_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_activs_and_weights\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m squared_error = \u001b[32m0\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(spd_model.num_layers):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/interp-replication/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/interp-replication/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[261]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mSPDModelMLP.forward\u001b[39m\u001b[34m(self, x, return_activs_and_weights, masks)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# code to make sure I'm not doing some goofy shit with the activations at the beginning\u001b[39;00m\n\u001b[32m    131\u001b[39m     check_layer_in_uv = torch.einsum(\u001b[33m\"\u001b[39m\u001b[33mnc,ch->nh\u001b[39m\u001b[33m\"\u001b[39m, v_activ_layer_in.squeeze(), \u001b[38;5;28mself\u001b[39m.U_in[l])\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m torch.allclose(layer_in, check_layer_in_uv, atol=\u001b[32m10\u001b[39m), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSubcomponent activations not calculated correctly, max difference is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.max(layer_in-check_layer_in_uv)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_in\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcheck_layer_in_uv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# would in theory have one for the second too but I'm being lazy\u001b[39;00m\n\u001b[32m    135\u001b[39m x_resid = x_resid + layer_out\n",
      "\u001b[31mAssertionError\u001b[39m: Subcomponent activations not calculated correctly, max difference is 9.9375; tensor([[1014606.5000, -265019.7188,  666373.5000,  ..., -427995.0312,\n          218633.4062, 1629082.1250],\n        [ -25435.8477,    7606.3452,   56588.8906,  ...,  -40041.9414,\n           33080.8945,  127081.9062],\n        [   9664.1396,  291722.1250, -148703.9219,  ...,  393290.3750,\n         -896824.0000, -758538.4375],\n        ...,\n        [-160292.5625, -126715.6875,  147662.5938,  ..., -216781.6094,\n          412628.8750,  442505.0625],\n        [-201789.5625,   40519.5156,  -18451.2734,  ...,  -56442.4766,\n          120751.6641,  -22576.2910],\n        [-158896.9219,  -89091.0234,  -94191.8281,  ...,  -27028.8262,\n          335013.8125,  -67209.3047]], grad_fn=<AddBackward0>)tensor([[1014615.6875, -265011.1562,  666366.3125,  ..., -427994.5000,\n          218640.9844, 1629091.2500],\n        [ -25427.0684,    7614.9150,   56581.7930,  ...,  -40041.4453,\n           33088.3633,  127091.1953],\n        [   9672.9824,  291730.6875, -148710.9219,  ...,  393291.0000,\n         -896816.5000, -758529.6875],\n        ...,\n        [-160283.7188, -126707.2266,  147655.5469,  ..., -216781.1094,\n          412636.5000,  442514.5938],\n        [-201780.8750,   40528.0820,  -18458.3379,  ...,  -56442.0156,\n          120759.2734,  -22567.0059],\n        [-158888.2188,  -89082.5234,  -94198.9766,  ...,  -27028.3184,\n          335021.4375,  -67200.0234]], grad_fn=<ViewBackward0>)"
     ]
    }
   ],
   "source": [
    "class SPDModelMLP(nn.Module): \n",
    "    # fix the betas stuff below that's basically a type hint\n",
    "    def __init__(self, target_model, config, device=\"cpu\"): \n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"Devices: \", device, target_model.device)\n",
    "        self.device = device\n",
    "        object.__setattr__(self, \"target_model\", target_model) # sets pointer to target_model without registering its parameters as subsidiary\n",
    "\n",
    "        self.target_model = target_model\n",
    "        assert self.device == target_model.device, \"Models not on same device\"\n",
    "\n",
    "        self.num_layers, self.pre_embed_size, self.in_size, self.hidden_size, self.imp_hidden_size = config[\"num_layers\"], config[\"pre_embed_size\"], config[\"in_size\"], config[\"hidden_size\"], config[\"importance_mlp_size\"]\n",
    "        self.C = config[\"subcomponents_per_layer\"]\n",
    "        self.hypers = dict(list(config.items())[4:]) # sets the \"hypers\" to contain all the hyperparameters for the model\n",
    "        \n",
    "        # Subcomponent vectors, each of shape C by in_size; to be used\n",
    "        # with outer product to create our low-rank subcomponent matrices\n",
    "        self.V_embed = nn.Parameter(torch.empty((self.C, self.pre_embed_size,), device=device))\n",
    "        self.U_embed = nn.Parameter(torch.empty((self.C, self.in_size,), device=device))\n",
    "        self.V_unembed = nn.Parameter(torch.empty((self.C, self.in_size,), device=device))\n",
    "        self.U_unembed = nn.Parameter(torch.empty((self.C, self.pre_embed_size,), device=device))\n",
    "\n",
    "        self.V_in = nn.ParameterList([torch.empty((self.C, self.in_size,), device=device) for i in range(self.num_layers)])\n",
    "        self.U_in = nn.ParameterList([torch.empty((self.C, self.hidden_size,), device=device) for i in range(self.num_layers)]) \n",
    "        self.V_out = nn.ParameterList([torch.empty((self.C, self.hidden_size,), device=device) for i in range(self.num_layers)])\n",
    "        self.U_out = nn.ParameterList([torch.empty((self.C, self.in_size,), device=device) for i in range(self.num_layers)])        \n",
    "        \n",
    "        # idk what you do with the biases lol\n",
    "        self.b = nn.ParameterList([torch.zeros((self.hidden_size,), device=device) for i in range(self.num_layers)])\n",
    "        \n",
    "        # this is so horrible I'm sorry\n",
    "        # gate_in_in is the gate_in weights for the in subcomponent of each layer\n",
    "        # gate_in_out is gate_in weights for the out component\n",
    "        # they each get an extra one on the end which is for the embed matrix\n",
    "        # Probably should have just registered these as submodels, but I didn't know that those existed before.\n",
    "        self.imp_W_gate_in_in = nn.ParameterList([torch.empty(self.C, 1, self.imp_hidden_size) for i in range(self.num_layers+1)])\n",
    "        self.imp_W_gate_out_in = nn.ParameterList([torch.empty(self.C, self.imp_hidden_size, 1) for i in range(self.num_layers+1)]) \n",
    "        self.imp_b_in_in = nn.ParameterList([torch.empty(self.C, self.imp_hidden_size) for i in range(self.num_layers+1)])\n",
    "        self.imp_b_out_in = nn.ParameterList([torch.empty(self.C, 1) for i in range(self.num_layers+1)])\n",
    "        \n",
    "        self.imp_W_gate_in_out = nn.ParameterList([torch.empty(self.C, 1, self.imp_hidden_size) for i in range(self.num_layers+1)])\n",
    "        self.imp_W_gate_out_out = nn.ParameterList([torch.empty(self.C, self.imp_hidden_size, 1) for i in range(self.num_layers+1)]) \n",
    "        self.imp_b_in_out = nn.ParameterList([torch.empty(self.C, self.imp_hidden_size) for i in range(self.num_layers+1)])\n",
    "        self.imp_b_out_out = nn.ParameterList([torch.empty(self.C, 1) for i in range(self.num_layers+1)])\n",
    "        \n",
    "        # self.pred_weights = torch.empty((num_layers, self.C), device=device)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            # using xavier anyway -- note that the variance etc is\n",
    "            # changed because we take the outer product in the \n",
    "            # forward pass\n",
    "            if param.dim() >= 2: # xavier does not work for 1d tensors\n",
    "                nn.init.xavier_normal_(param)\n",
    "            else:\n",
    "                nn.init.zeros_(param)  # or another appropriate initializer for biases\n",
    "\n",
    "        \n",
    "    def forward(self, x, return_activs_and_weights=False, masks=None): # Regular run. Unclear whether I should have masking when I do a regular forward pass.\n",
    "        v_activations = []\n",
    "        weight_matrices = []\n",
    "        x_in = x.clone()\n",
    "        layerwise_resids = []\n",
    "\n",
    "        if masks is not None: \n",
    "            \n",
    "            v_activ_embed = torch.einsum(\"np,cp->nc\", x_in, self.V_embed) * masks[-1][\"embed\"] # TODO: ADD MASKS FOR EMBED\n",
    "            x_embedded = torch.einsum(\"nc,ci->ni\", v_activ_embed, self.U_embed)\n",
    "            x_resid = x_embedded.clone()\n",
    "                        \n",
    "            for l in range(self.num_layers):\n",
    "                ### Run the forward pass just for this layer\n",
    "                v_activ_layer_in = torch.einsum(\"ni,ci->nc\", x_resid, self.V_in[l]) * masks[l][\"in\"]\n",
    "                layer_hidden = F.relu(torch.einsum(\"nc,ch->nh\", v_activ_layer_in, self.U_in[l]) + self.b[l])\n",
    "\n",
    "                v_activ_layer_out = torch.einsum(\"nh,ch->nc\", layer_hidden, self.V_out[l]) * masks[l][\"out\"]\n",
    "                layer_out = torch.einsum(\"nc,ci->ni\", v_activ_layer_out, self.U_out[l])\n",
    "                x_resid = x_resid + layer_out\n",
    "\n",
    "                # Run the layerwise forward pass for the whole model, with masking only this layer\n",
    "                x_resid_layerwise = x_embedded.clone() # pre-embedded\n",
    "                for l_2 in range(self.num_layers): # oh god this is so awful. literally sobbing rn. i can't believe I'm writing it like this\n",
    "                    if l_2 == l: \n",
    "                        v_activ_layer_in_l = torch.einsum(\"ni,ci->nc\", x_resid_layerwise, self.V_in[l]) * masks[l][\"in\"]\n",
    "                        layer_hidden_l = F.relu(torch.einsum(\"nc,ch->nh\", v_activ_layer_in_l, self.U_in[l])+self.b[l])\n",
    "                        v_activ_layer_out_l = torch.einsum(\"nh,ch->nc\", layer_hidden_l, self.V_out[l]) * masks[l][\"out\"]\n",
    "                        layer_out_l = torch.einsum(\"nc,ci->ni\", v_activ_layer_out_l, self.U_out[l])\n",
    "                    else: # THE SAME EXACT THING EXCEPT WITHOUT THE MASKS >_< \n",
    "                        v_activ_layer_in_l = torch.einsum(\"ni,ci->nc\", x_resid_layerwise, self.V_in[l])\n",
    "                        layer_hidden_l = torch.einsum(\"nc,ch->nh\", v_activ_layer_in_l, self.U_in[l])\n",
    "                        # ReLU and bias here?\n",
    "                        v_activ_layer_l = torch.einsum(\"nh,ch->nc\", layer_in_l, self.V_out[l])\n",
    "                        layer_out_l = torch.einsum(\"nc,ci->ni\", v_activ_layer_in_l, self.U_out[l])\n",
    "                    x_resid_layerwise = x_resid_layerwise + layer_out_l\n",
    "                x_out_layerwise = torch.einsum(\"nc,cp->np\", torch.einsum(\"ni,ci->nc\", x_resid, self.V_unembed) * masks[-1][\"unembed\"], self.U_unembed)\n",
    "                layerwise_resids.append(x_out_layerwise)\n",
    "            \n",
    "            v_activ_unembed = torch.einsum(\"ni,ci->nc\", x_resid, self.V_unembed) * masks[-1][\"unembed\"]\n",
    "            x_out = torch.einsum(\"nc,cp->np\", v_activ_unembed, self.U_unembed)\n",
    "\n",
    "                                \n",
    "        else: \n",
    "            W_embed = torch.einsum(\"cp, ci -> cpi\", self.V_embed, self.U_embed).sum(dim=0)\n",
    "            x_resid = torch.einsum(\"np, pi -> ni\", x_in, W_embed)\n",
    "            v_activ_embed = torch.einsum(\"np,cp->nc\", x_in, self.V_embed).unsqueeze(-1)\n",
    "            \n",
    "            for l in range(self.num_layers):\n",
    "                # Use outer product to create weights for the layer, then sum all the subcomponents\n",
    "                W_in = torch.einsum(\"ci,ch-> cih\", self.V_in[l], self.U_in[l]).sum(dim=0) # shape i h\n",
    "                W_out = torch.einsum(\"ch,ci-> chi\", self.V_out[l], self.U_out[l]).sum(dim=0) # shape h i\n",
    "                if return_activs_and_weights == True:\n",
    "                    weight_matrices.append({\"in\": W_in, \"out\": W_out})\n",
    "                # COMPUTE \n",
    "                layer_in = torch.einsum(\"ni,ih -> nh\", x_resid, W_in) + self.b[l]\n",
    "                layer_out = torch.einsum(\"nh,hi->ni\", F.relu(layer_in), W_out)\n",
    "            \n",
    "            if return_activs_and_weights == True: \n",
    "                # calculate activations\n",
    "                v_activ_layer_in = torch.einsum(\"ni,ci->nc\", x_resid, self.V_in[l]).unsqueeze(-1)\n",
    "                v_activ_layer_out = torch.einsum(\"nh,ch->nc\", layer_in, self.V_out[l]).unsqueeze(-1)\n",
    "                # both are now shape n,c,1\n",
    "                v_activations.append({\"in\": v_activ_layer_in, \"out\": v_activ_layer_out})\n",
    "                \n",
    "                # code to make sure I'm not doing some goofy shit with the activations at the beginning\n",
    "                check_layer_in_uv = torch.einsum(\"nc,ch->nh\", v_activ_layer_in.squeeze(), self.U_in[l])\n",
    "                assert torch.allclose(layer_in, check_layer_in_uv, atol=10), f\"Subcomponent activations not calculated correctly, max difference is {torch.max(layer_in-check_layer_in_uv)}; {layer_in}{check_layer_in_uv}\"\n",
    "                # would in theory have one for the second too but I'm being lazy\n",
    "            \n",
    "            x_resid = x_resid + layer_out\n",
    "            W_unembed = torch.einsum(\"ci, cp-> cip\", self.V_unembed, self.U_unembed).sum(dim=0)\n",
    "            x_out = torch.einsum(\"ni, ip -> np\", x_resid, W_unembed)\n",
    "            weight_matrices.append({\"embed\": W_embed, \"unembed\": W_unembed})\n",
    "            \n",
    "            v_activ_unembed = torch.einsum(\"ni,ci->nc\", x_resid, self.V_unembed).unsqueeze(-1)\n",
    "            v_activations.append({\"embed\": v_activ_embed, \"unembed\": v_activ_unembed})\n",
    "\n",
    "            # putting the embed matrices at the end of the masks. since we iterate through l in range(num_layers) this will not get added! :D\n",
    "\n",
    "        if return_activs_and_weights:\n",
    "            return x_out, v_activations, weight_matrices \n",
    "        elif masks is not None: \n",
    "            return x_out, layerwise_resids\n",
    "        else: \n",
    "            return x_out\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Config\n",
    "    #device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "    config = {\n",
    "        \"num_layers\": 1,\n",
    "        \"pre_embed_size\": 100,\n",
    "        \"in_size\": 1000,\n",
    "        \"hidden_size\": 50,\n",
    "        \"subcomponents_per_layer\": 10, \n",
    "        \"beta_1\": 1.0, \n",
    "        \"beta_2\": 1.0, \n",
    "        \"beta_3\": 1.0, \n",
    "        \"causal_imp_min\": 1.0, \n",
    "        \"num_mask_samples\": 20,\n",
    "        \"importance_mlp_size\": 5,\n",
    "    }\n",
    "    \n",
    "    dataset = SparseAutoencoderDataset(\n",
    "        in_dim=100,\n",
    "        n_samples=10000,\n",
    "        sparsity=0.9,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    # Model\n",
    "    spd_model = SPDModelMLP(toy_model, config, device=device)\n",
    "    # Train\n",
    "    train_SPD(spd_model, dataloader, lr=8e-2, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6731eff5-4248-4332-85c5-ca3afc41f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SPD(spd_model, dataloader, lr=1e-3, num_epochs=10): # could also implement this by passing in the original model?\n",
    "\n",
    "    model.train()\n",
    "    print(f\"Training on device {spd_model.device}\")\n",
    "    optimizer = torch.optim.AdamW(spd_model.parameters(), lr = lr)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_l_stoch_rec, total_l_stoch_rec_l, total_l_imp, total_l_faith = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as t:\n",
    "            for x,y in t:\n",
    "                \n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                N = x.shape[0]     # x is shape N by in_size\n",
    "                C = spd_model.C\n",
    "                target_model = spd_model.target_model\n",
    "                hard_sigmoid = HardSigmoid()\n",
    "                \n",
    "                # COMPUTE TARGET OUTPUT\n",
    "                with torch.no_grad(): \n",
    "                    target_out = spd_model.target_model(x)\n",
    "            \n",
    "                # FAITHFULNESS LOSS\n",
    "                \n",
    "                spd_output, spd_activations, spd_weights = spd_model(x, return_activs_and_weights = True)\n",
    "                squared_error = 0\n",
    "                for l in range(spd_model.num_layers):\n",
    "                    in_diff = target_model.W_in[l] - spd_weights[l][\"in\"]\n",
    "                    out_diff = target_model.W_out[l] - spd_weights[l][\"out\"]\n",
    "                    \n",
    "                    # torch.linalg.matrix_norm defaults to the frobenius norm\n",
    "                    # this takes the frobenius norm of the diff and then squares it\n",
    "                    squared_error_layer = torch.linalg.matrix_norm(in_diff) ** 2 + torch.linalg.matrix_norm(out_diff) ** 2 \n",
    "                    squared_error = squared_error + squared_error_layer\n",
    "                    \n",
    "                mean_squared_error = squared_error/spd_model.num_layers\n",
    "                l_faithfulness = mean_squared_error\n",
    "                \n",
    "            \n",
    "                ## IMPORTANCE-MINIMALITY LOSS\n",
    "                \n",
    "                pred_importances = []\n",
    "                l_importance_minimality = 0.0\n",
    "    \n",
    "                components_imp_pred_embed_hidden = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[-1][\"embed\"], spd_model.imp_W_gate_in_in[-1]) + spd_model.imp_b_in_in[-1])\n",
    "                components_imp_pred_embed = hard_sigmoid(torch.einsum(\"ncs,cso->nco\", components_imp_pred_embed_hidden, spd_model.imp_W_gate_out_in[-1]) + spd_model.imp_b_out_in[-1])\n",
    "                \n",
    "                components_imp_pred_unembed_hidden = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[-1][\"unembed\"], spd_model.imp_W_gate_in_out[-1]) + spd_model.imp_b_in_out[-1])\n",
    "                components_imp_pred_unembed = hard_sigmoid(torch.einsum(\"ncs,cso->nco\", components_imp_pred_unembed_hidden, spd_model.imp_W_gate_out_out[-1]) + spd_model.imp_b_out_out[-1])\n",
    "                l_importance_minimality = l_importance_minimality + (components_imp_pred_embed.sum() ** spd_model.hypers[\"causal_imp_min\"]) + (components_imp_pred_unembed.sum() ** spd_model.hypers[\"causal_imp_min\"])\n",
    "            \n",
    "                for l in range(spd_model.num_layers):\n",
    "                    # both activations are n by c containing dot product so we already have hard_sigmoid\n",
    "                    # spd_activations[l][inout] is shape n,c,1 (nco)\n",
    "                    # imp_W_in is c by 1 by imp_size (cos)\n",
    "                    # want to map to ncs then back to nco\n",
    "                    # imp_b_in is shape (C, s) so should broadcast to ncs nicely\n",
    "                    \n",
    "                    # TODO: DEFINE hard_sigmoid AS A TORCH MODULE SO IT CAN CALCULATE THE DERIVATIVE \n",
    "                    # in theory should write this as a bunch of models stored in the main model, but that's not how i did it and i've already \n",
    "                    # written this, shrug\n",
    "                    # oh my god this is so bad \n",
    "                    # imp_W_gate_in_in is the gate in for the W_in, etc\n",
    "                    components_imp_pred_hidden_in = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[l][\"in\"], spd_model.imp_W_gate_in_in[l]) + spd_model.imp_b_in_in[l])\n",
    "                    components_pred_layer_in = hard_sigmoid(torch.einsum(\"ncs,cso->nco\", components_imp_pred_hidden_in, spd_model.imp_W_gate_out_in[l]) + spd_model.imp_b_out_in[l])\n",
    "            \n",
    "                    #same thing for the out matrix in layer l\n",
    "                    components_imp_pred_hidden_out = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[l][\"out\"], spd_model.imp_W_gate_in_out[l]) + spd_model.imp_b_in_out[l])\n",
    "                    components_pred_layer_out = hard_sigmoid(torch.einsum(\"ncs,cso->nco\", components_imp_pred_hidden_out, spd_model.imp_W_gate_out_out[l]) + spd_model.imp_b_out_out[l])\n",
    "                    \n",
    "                    pred_importances.append({\"in\": components_pred_layer_in, \"out\": components_pred_layer_out})\n",
    "                    l_importance_minimality = l_importance_minimality + (components_pred_layer_in.sum() ** spd_model.hypers[\"causal_imp_min\"]) + (components_pred_layer_out.sum() ** spd_model.hypers[\"causal_imp_min\"])\n",
    "                \n",
    "                pred_importances.append({\"embed\": components_imp_pred_embed, \"unembed\": components_imp_pred_unembed})\n",
    "    \n",
    "                l_importance_minimality /= N # divide by N, the batch size, to avg across the batch (notated as B in the paper)\n",
    "            \n",
    "            \n",
    "                ## STOCHASTIC RECONSTRUCTION LOSS \n",
    "            \n",
    "                l_stochastic_recon = 0\n",
    "                l_stochastic_recon_layerwise = 0\n",
    "                R = torch.rand(spd_model.hypers[\"num_mask_samples\"], N, spd_model.num_layers+1, 2, C)\n",
    "                layer_masks = []\n",
    "            \n",
    "                for s in range(spd_model.hypers[\"num_mask_samples\"]):\n",
    "                    # Running this with a for loop. This is slow; I'd ideally just run \n",
    "                    # something like M = G[None, :, :] + (1 - G[None, :, :]) * R\n",
    "                    # But I think there's a clarity tradeoff here so I'm just going to do this for now\n",
    "                    layer_mask_embed = pred_importances[-1][\"embed\"].squeeze() + (torch.ones_like(pred_importances[-1][\"embed\"].squeeze()) - pred_importances[-1][\"embed\"].squeeze())*R[s,:,l,0,:]\n",
    "                    layer_mask_unembed = pred_importances[-1][\"unembed\"].squeeze() + (torch.ones_like(pred_importances[-1][\"unembed\"].squeeze()) - pred_importances[-1][\"unembed\"].squeeze()) * R[s,:,l,1,:]\n",
    "                    \n",
    "                    for l in range(spd_model.num_layers):\n",
    "                        # ugh, this is pretty bad. Ideally I'd go back and refactor so that I don't \n",
    "                        # have separate in and out weights, but this is what I have. going to keep it\n",
    "                        # like this for now, but will refactor once I have something that works.\n",
    "                        # components pred in is shape N, C, 1, squeeze to N, C. there are L of them\n",
    "                        # R is N, L, C, sample just l to get N, C\n",
    "                        # I want masks for each component on each layer on each datapoint, so\n",
    "                        # masks should be one C vector for each layer, per-datapoint. so NLC\n",
    "    \n",
    "                        # whoops i'm now putting squeezes on everything. seems like I should just unsqueeze the thing when I store it, but I don't feel like refactoring it like that :( \n",
    "                        layer_mask_in = pred_importances[l][\"in\"].squeeze() + (torch.ones_like(pred_importances[l][\"in\"].squeeze()) - pred_importances[l][\"in\"].squeeze()) * R[s,:,l,0,:]\n",
    "                        layer_mask_out = pred_importances[l][\"out\"].squeeze() + (torch.ones_like(pred_importances[l][\"out\"].squeeze()) - pred_importances[l][\"out\"].squeeze()) * R[s,:,l,1,:]\n",
    "                        # these are both shape N, C \n",
    "                        layer_masks.append({\"in\": layer_mask_in, \"out\": layer_mask_out})\n",
    "                        \n",
    "                    layer_masks.append({\"embed\": layer_mask_embed, \"unembed\": layer_mask_unembed})\n",
    "                    masked_out, layerwise_masked_outs = spd_model(x, masks=layer_masks)\n",
    "                    l_stochastic_recon = l_stochastic_recon + torch.linalg.matrix_norm(target_out - masked_out) # uses matrix norm of difference\n",
    "                    for i in range(len(layerwise_masked_outs)-1):\n",
    "                        l_stochastic_recon_layerwise = l_stochastic_recon_layerwise + torch.linalg.matrix_norm(target_out - layerwise_masked_outs[i])\n",
    "            \n",
    "                l_stochastic_recon /= spd_model.hypers[\"num_mask_samples\"]\n",
    "                l_stochastic_recon_layerwise /= (spd_model.hypers[\"num_mask_samples\"] * spd_model.num_layers)\n",
    "                \n",
    "                loss = l_faithfulness + spd_model.hypers[\"beta_1\"] * l_stochastic_recon + spd_model.hypers[\"beta_2\"] * l_stochastic_recon_layerwise + spd_model.hypers[\"beta_3\"] * l_importance_minimality\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                \n",
    "                total_l_faith += l_faithfulness\n",
    "                total_l_stoch_rec +=l_stochastic_recon\n",
    "                total_l_stoch_rec_l += l_stochastic_recon_layerwise\n",
    "                total_l_imp += l_importance_minimality\n",
    "                \n",
    "                t.set_postfix(loss=loss.item())\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        total_l_stoch_rec, total_l_stoch_rec_l, total_l_imp, total_l_faith = total_l_stoch_rec/len(dataloader.dataset), total_l_stoch_rec_l/len(dataloader.dataset), total_l_imp/len(dataloader.dataset), total_l_faith/len(dataloader.dataset)\n",
    "        print(f\"Epoch {epoch+1}: avg loss = {avg_loss:.6f}\")\n",
    "        print(total_l_faith.item(), total_l_stoch_rec.item(), total_l_stoch_rec_l, total_l_imp.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-replication",
   "language": "python",
   "name": "interp-replication"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
