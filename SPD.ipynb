{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5b19f-26f8-4a66-85ca-5f81c949767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the torch stuff, as well as the ViT stuff and such\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c791fb62-d6be-4926-b345-c518a5239fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import einops\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ea7aa-613d-4354-95c0-44fbdcc7849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your original toy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48df6266-0231-4c6d-a813-19f4bf12553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SPDHookedTransformer Class\n",
    "# that extends the hookedTransformer\n",
    "# which inherits all the previous class stuff\n",
    "# but also contains the SPD training algo?\n",
    "\n",
    "# how is everything actually structured? \n",
    "\"\"\"\n",
    "Maybe I should start with implementing it on a simple MLP setup. \n",
    "\n",
    "Orig network:\n",
    "- 1 layer MLP 5-2-5 (defined as usual with torch.Sequential presumably)\n",
    "\n",
    "SPD network:\n",
    "let's give it 10 subcomponents per layer\n",
    "then it's just like 10 matmuls? \n",
    "maybe i should define it like ant did in the toy models of superposition paper\n",
    "stick them all into a trenchcoat\n",
    "this might be easier once I have defined the toy modle\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50be6f25-5489-4445-82c0-914779cc7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_layers\": 2,\n",
    "    \"in_size\": 10,\n",
    "    \"hidden_size\": 5,\n",
    "    \"subcomponents_per_layer\": 5, \n",
    "    \"beta_1\": 1, \n",
    "    \"beta_2\": 1, \n",
    "    \"beta_3\": 1, \n",
    "    \"causal_imp_min\": 1, \n",
    "    \"num_samples\": 10,\n",
    "    \"importance_mlp_size\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a426a65-b7dc-4e68-9e35-da69dbdf7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyResidMLP(nn.Module):\n",
    "    def __init__(self, config, device=\"mps\"):\n",
    "        super().__init__()\n",
    "        # Initialize Weights for the\n",
    "        self.num_layers, self.in_size, self.hidden_size = config[\"num_layers\"], config[\"in_size\"], config[\"hidden_size\"]\n",
    "        self.device = device\n",
    "        self.W_in = nn.ParameterList([torch.empty((in_size, hidden_size), device=device) for i in range(num_layers)])\n",
    "        self.W_out = nn.ParameterList([torch.empty((hidden_size, in_size), device=device) for i in range(num_layers)])\n",
    "        self.b = nn.ParameterList([torch.zeros((hidden_size,), device=device) for i in range(num_layers)])\n",
    "\n",
    "        for param in self.W_in + self.W_out: \n",
    "            nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        # apply matmul lol\n",
    "        N, D = x.shape\n",
    "        assert D == self.in_size, f\"Input shape does not match model's accepted size {self.in_size}\"\n",
    "        # something to ensure shape?\n",
    "        \n",
    "        x_resid = x\n",
    "        for i in range(self.num_layers):\n",
    "            hidden = F.relu(torch.einsum(\"nd,dh -> nh\", x_resid, self.W_in[i]))\n",
    "            layer_out = torch.einsum(\"nh,hd -> nd\", hidden, self.W_out[i])\n",
    "            x_resid += layer_out\n",
    "        # am I supposed to have a embed and out?\n",
    "        return x_resid\n",
    "\n",
    "\n",
    "def toy_train(model, lr, num_steps): \n",
    "    # init AdamW optimizer on model\n",
    "    # wrap train function for tqdm\n",
    "    # for step in num_step:\n",
    "    # generate batch for the step\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50310679-3db3-4b7d-8a8b-24c32dc176e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPDModelMLP(nn.Module): \n",
    "    # fix the betas stuff below that's basically a type hint\n",
    "    def __init__(self, target_model, config, device=\"mps\"): \n",
    "        self.device = device\n",
    "        self.target_model = target_model\n",
    "        self.num_layers, self.in_size, self.hidden_size, self.imp_hidden_size = config[\"num_layers\"], config[\"in_size\"], config[\"hidden_size\"], config[\"importance_mlp_size\"]\n",
    "        assert self.device == target_model.device, \"Models not on same device\"\n",
    "        self.C = config[\"subcomponents_per_layer\"]\n",
    "        self.hypers = dict(list(config.items())[4:]) # sets the \"hypers\" to contain all the hyperparameters for the model\n",
    "        \n",
    "        # Subcomponent vectors, each of shape C by in_size; to be used\n",
    "        # with outer product to create our low-rank subcomponent matrices\n",
    "        self.V_in = nn.ParameterList([torch.empty((self.C, in_size,), device=device) for i in range(num_layers)])\n",
    "        self.U_in = nn.ParameterList([torch.empty((self.C, hidden_size,), device=device) for i in range(num_layers)]) \n",
    "        self.V_out = nn.ParameterList([torch.empty((self.C, hidden_size,), device=device) for i in range(num_layers)])\n",
    "        self.U_out = nn.ParameterList([torch.empty((self.C, in_size,), device=device) for i in range(num_layers)])\n",
    "        \n",
    "        # idk what you do with the biases lol\n",
    "        self.b = nn.ParameterList([torch.zeros((hidden_size,), device=device) for i in range(num_layers)])\n",
    "        \n",
    "        # imp_W_in and out etc are the weights for the importance predictor\n",
    "        # should be C networks per layer, mapping 1 -> C -> 1\n",
    "        self.imp_W_gate_in = nn.ParameterList([torch.empty(C, 1, self.imp_hidden_size) for i in range(num_layers)])\n",
    "        self.imp_W_gate_out = nn.ParameterList([torch.empty(C, self.imp_hidden_size, 1) for i in range(num_layers)])\n",
    "        self.imp_b_in = nn.ParameterList([torch.empty(C, self.imp_hidden_size) for i in range(num_layers)])\n",
    "        self.imp_b_out = nn.ParameterList([torch.empty(C, 1) for i in range(num_layers)])\n",
    "        \n",
    "        \n",
    "        # self.pred_weights = torch.empty((num_layers, self.C), device=device)\n",
    "\n",
    "        for param in self.V_in + self.U_in + self.V_out + self.U_out + self.b: \n",
    "            # using xavier anyway -- note that the variance etc is\n",
    "            # changed because we take the outer product in the \n",
    "            # forward pass\n",
    "            nn.init.xavier_normal_(param)\n",
    "\n",
    "        \n",
    "    def forward_with_activations(self, x): # Regular run. Unclear whether I should have masking when I do a regular forward pass.\n",
    "        v_activations = []\n",
    "        weight_matrices = []\n",
    "        x_resid = x\n",
    "        \n",
    "        for i in self.num_layers:\n",
    "\n",
    "            # Use outer product to create weights for the layer, then sum all the subcomponents\n",
    "            W_in = torch.einsum(\"ci,ch-> cih\", self.V_in[i], self.U_in[i]).sum(dim=0) # shape i h\n",
    "            W_out = torch.einsum(\"ch,ci-> chi\", self.V_out[i], self.U_out[i]).sum(dim=0) # shape h i\n",
    "            weight_matrices.append({\"in\": W_in, \"out\", W_out})\n",
    "\n",
    "            layer_in = torch.einsum(\"ni,ih -> nh\", x_resid, W_in) + self.b[i]\n",
    "            layer_out = torch.einsum(\"nh,hi->ni\", F.relu(layer_in), W_out)\n",
    "\n",
    "            # calculate activations\n",
    "            v_activ_layer_in = torch.einsum(\"ni,ci->nc\", x_resid, self.V_in[i]).unsqueeze(-1)\n",
    "            v_activ_layer_out = torch.einsum(\"nh,ch->nc\", layer_in, self.V_out[i]).unsqueeze(-1)\n",
    "            # both are now shape n,c,1\n",
    "            v_activations.append({\"in\": v_activ_layer_in, \"out\": v_activ_layer_out})\n",
    "            \n",
    "            # code to make sure I'm not doing some goofy shit with the activations at the beginning\n",
    "            check_layer_in_uv = torch.einsum(\"nc,ch->nh\", v_activ_layer_in, self.U_in[i])\n",
    "            assert layer_in == check_layer_in_uv, \"Subcomponent activations not calculated correctly\"\n",
    "            # would in theory have one for the second too but I'm being lazy\n",
    "            \n",
    "            x_resid += layer_out\n",
    "        return x_resid, v_activations, weight_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731eff5-4248-4332-85c5-ca3afc41f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(config): \n",
    "    # return shape N, config.in_size \n",
    "    pass\n",
    "\n",
    "def train_SPD(spd_model): # could also implement this by passing in the original model\n",
    "    # generate batch someho\n",
    "    x = generate_batch # TODO (above). possibly put in the model? idk if needed.\n",
    "    N = x.shape[0]\n",
    "    # x is shape N by in_size\n",
    "    target_model = spd_model.target_model\n",
    "    \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        target_out = spd_model.target_model(x)\n",
    "\n",
    "    # MSE Loss\n",
    "    spd_output, spd_activations spd_weights = model.forward_with_activations(x)\n",
    "    squared_error = 0\n",
    "    for i in range(num_layers):\n",
    "        in_diff = target_model.W_in[i] - spd_weights[i][\"in\"]\n",
    "        out_diff = target_model.W_out[i] - spd_weights[i][\"out\"]\n",
    "        \n",
    "        # torch.linalg.matrix_norm defaults to the frobenius norm\n",
    "        # this takes the frobenius norm of the diff and then squares it\n",
    "        squared_error_layer = torch.linalg.matrix_norm(in_diff) ** 2 + torch.linalg.matrix_norm(out_diff) ** 2 \n",
    "        squared_error += squared_error_layer\n",
    "        \n",
    "    mean_squared_error = squared_error/num_layers\n",
    "    l_faithfulness = mean_squared_error\n",
    "\n",
    "    # Predict importances, and just add to importance_minimality loss for the sake of compactness; we'd otherwise need two for loops to implement\n",
    "    pred_importances = []\n",
    "    l_importance_minimality = 0\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        # both activations are n by c containing dot product so we already have hard_sigmoid\n",
    "        # spd_activations[i][inout] is shape n,c,1 (nco)\n",
    "        # imp_W_in is c by 1 by imp_size (cos)\n",
    "        # want to map to ncs then back to nco\n",
    "        # imp_b_in is shape (C, s) so should broadcast to ncs nicely\n",
    "        \n",
    "        # TODO: DEFINE hard_sigmoid AS A TORCH MODULE SO IT CAN CALCULATE THE DERIVATIVE \n",
    "        # in theory should write this as a bunch of models stored in the main model, but that's not how i did it and i've already \n",
    "        # written this, shrug\n",
    "        components_imp_pred_hidden_in = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[i][\"in\"], spd_model.imp_W_gate_in) + spd_model.imp_b_in)\n",
    "        components_pred_layer_in = hard_sigmoid(torch.einsum(\"ncs,cso->nco\", components_imp_pred_hidden, spd_model.imp_W_gate_out) + spd_model.imp_b_out)\n",
    "\n",
    "        #same thing for the out matrix in layer l\n",
    "        components_imp_pred_hidden_out = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[i][\"out\"], spd_model.imp_W_gate_in) + spd_model.imp_b_in)\n",
    "        components_pred_layer_out = hard_sigmoid(torch.einsum(\"ncs,cso->nco\", components_imp_pred_hidden, spd_model.imp_W_gate_out) + spd_model.imp_b_out)\n",
    "        \n",
    "        pred_importances.append({\"in\": components_pred_layer_in, \"out\": components_pred_layer_out)\n",
    "        l_importance_minimality += (components_pred_layer_in ** spd_model.hypers[\"importance_mlp_size\"]) + (components_pred_layer_out ** spd_model.hypers[\"importance_mlp_size\"])\n",
    "    \n",
    "    l_importance_minimality /= N # divide by N, the batch size, to avg across the batch (notated as B in the paper)\n",
    "\n",
    "    l_stochastic_recon = 0\n",
    "    l_stochastic_recon = 0\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: \n",
    "    - define hard_sigmoid\n",
    "    - generate batches\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351b0fc-8336-4d5d-86c5-70c6193844d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward pass is like\n",
    "0. generate or sample datafs\n",
    "1. run the regular model\n",
    "2. run SPD model\n",
    "3. faithfulness loss (check that the SPD model weights sum to the original model)\n",
    "4. Get 'intermediate activations' and then get MLP predictions for each layer\n",
    "5. compute importance-minimality loss\n",
    "6. Sample Rs and compute masked weights\n",
    "7. run model with random masking, with \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-replication",
   "language": "python",
   "name": "interp-replication"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
