{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374687f7-c6bd-462e-92cb-95a74322304b",
   "metadata": {},
   "source": [
    "# Stochastic Parameter Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4be581-416b-4231-b780-c5cabb7142f6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5b19f-26f8-4a66-85ca-5f81c949767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the torch stuff, as well as the ViT stuff and such\n",
    "# import plotly.express as px\n",
    "# import torch\n",
    "# from jaxtyping import Int, Float\n",
    "# from typing import List, Optional, Tuple\n",
    "# from tqdm import tqdm\n",
    "# from transformer_lens.hook_points import HookPoint\n",
    "# from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "# import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c791fb62-d6be-4926-b345-c518a5239fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import einops\n",
    "import typing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd14d16-8e3e-4efe-9d5f-7f29610d6b83",
   "metadata": {},
   "source": [
    "## Toy Model MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48df6266-0231-4c6d-a813-19f4bf12553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SPDHookedTransformer Class\n",
    "# that extends the hookedTransformer\n",
    "# which inherits all the previous class stuff\n",
    "# but also contains the SPD training algo?\n",
    "\n",
    "# how is everything actually structured? \n",
    "\"\"\"\n",
    "Maybe I should start with implementing it on a simple MLP setup. \n",
    "\n",
    "Orig network:\n",
    "- 1 layer MLP 5-2-5 (defined as usual with torch.Sequential presumably)\n",
    "\n",
    "SPD network:\n",
    "let's give it 10 subcomponents per layer\n",
    "then it's just like 10 matmuls? \n",
    "maybe i should define it like ant did in the toy models of superposition paper\n",
    "stick them all into a trenchcoat\n",
    "this might be easier once I have defined the toy modle\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "5a426a65-b7dc-4e68-9e35-da69dbdf7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyResidMLP(nn.Module):\n",
    "    def __init__(self, config, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        # Initialize Weights for the\n",
    "        self.num_layers, self.pre_embed_size, self.in_size, self.hidden_size = config[\"num_layers\"], config[\"pre_embed_size\"], config[\"in_size\"], config[\"hidden_size\"]\n",
    "        self.device = device\n",
    "        self.W_embed = nn.Parameter(torch.empty((self.pre_embed_size, self.in_size)))\n",
    "        self.W_unembed = nn.Parameter(torch.empty((self.in_size, self.pre_embed_size)))\n",
    "        self.W_in = nn.ParameterList([torch.empty((self.in_size, self.hidden_size), device=device) for i in range(self.num_layers)])\n",
    "        self.W_out = nn.ParameterList([torch.empty((self.hidden_size, self.in_size), device=device) for i in range(self.num_layers)])\n",
    "        self.b = nn.ParameterList([torch.zeros((self.hidden_size,), device=device) for i in range(self.num_layers)])\n",
    "\n",
    "        for param in [self.W_embed, self.W_unembed] + list(self.W_in) + list(self.W_out): \n",
    "            nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        \n",
    "        assert x.shape[1] == self.pre_embed_size, f\"Input shape {x.shape[0]} does not match model's accepted size {self.pre_embed_size}\"\n",
    "        # embed \n",
    "        x_resid = torch.einsum(\"np,pi->ni\", x.clone(), self.W_embed)\n",
    "        N, D = x_resid.shape\n",
    "\n",
    "        \n",
    "        for l in range(self.num_layers):\n",
    "            hidden = F.relu(torch.einsum(\"nd,dh -> nh\", x_resid, self.W_in[l]) + self.b[l])\n",
    "            layer_out = torch.einsum(\"nh,hd -> nd\", hidden, self.W_out[l])\n",
    "            x_resid = x_resid + layer_out\n",
    "        # am I supposed to have a embed and out?\n",
    "        x_out = torch.einsum(\"ni,ip->np\", x_resid, self.W_unembed) \n",
    "        return x_out\n",
    "\n",
    "\n",
    "# Generated by LLM bc I am lazy and this is not important to me learning stuff atm\n",
    "class SparseAutoencoderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for learning to reconstruct sparse inputs.\n",
    "    Each item is (input, target) where target is the input itself (or ReLU of input).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=100, n_samples=10000, sparsity=0.9, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.n_samples = n_samples\n",
    "        self.device = device\n",
    "\n",
    "        # Pre-generate all samples\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Sparse input: each entry is -1, 0, or 1, with sparsity\n",
    "            x = np.random.choice([0, -1, 1], size=(in_dim,), p=[1-sparsity, sparsity/2, sparsity/2])\n",
    "            x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "            target = F.relu(x)\n",
    "            \n",
    "            self.inputs.append(x)\n",
    "            self.targets.append(target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "def train_toy_resid_mlp(\n",
    "    model,\n",
    "    dataloader,\n",
    "    lr=1e-3,\n",
    "    num_epochs=10,\n",
    "    device=\"cpu\",\n",
    "    print_every=1\n",
    "):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for x, y in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}: avg MSE loss = {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "d67d9bbc-74ee-4eca-8a38-90c245095172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 114.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg MSE loss = 531428.219825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 114.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg MSE loss = 111.851628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 114.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: avg MSE loss = 38.459008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 113.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg MSE loss = 18.165742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 113.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: avg MSE loss = 9.771155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 114.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: avg MSE loss = 5.692220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 114.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: avg MSE loss = 3.512130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 114.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: avg MSE loss = 2.284967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 114.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: avg MSE loss = 1.558404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 114.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: avg MSE loss = 1.116323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 112.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: avg MSE loss = 0.843962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 113.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: avg MSE loss = 0.669232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 114.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: avg MSE loss = 0.562225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 114.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: avg MSE loss = 0.494167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 113.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: avg MSE loss = 0.451150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 103.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: avg MSE loss = 0.425610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 113.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: avg MSE loss = 0.411201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 113.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: avg MSE loss = 0.404142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 113.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: avg MSE loss = 0.402326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 111.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: avg MSE loss = 0.403826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## LLM-Generated Usage Example\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Config\n",
    "    # device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "    # Dataset and DataLoader\n",
    "    # Dataset and DataLoader\n",
    "    dataset = SparseAutoencoderDataset(\n",
    "        in_dim=100,\n",
    "        n_samples=15000,\n",
    "        sparsity=0.9,\n",
    "        device=device,\n",
    "    )\n",
    "    print(device)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    # Model\n",
    "    toy_model = ToyResidMLP(config, device=device)\n",
    "    # Train\n",
    "    train_toy_resid_mlp(toy_model, dataloader, lr=8e-2, num_epochs=20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c72b4-6661-4d00-b27d-cee3f7ac01ac",
   "metadata": {},
   "source": [
    "## SPD Model and Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "b9bda061-7487-44ce-9201-dc1f6a34b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([-2.0000, -0.5000,  0.0000,  0.3000,  0.7000,  1.0000,  1.5000,  2.0000])\n",
      "Lower-leaky: tensor([-0.0200, -0.0050,  0.0000,  0.3000,  0.7000,  1.0000,  1.0000,  1.0000])\n",
      "Upper-leaky: tensor([0.0000, 0.0000, 0.0000, 0.3000, 0.7000, 1.0000, 1.0050, 1.0100])\n"
     ]
    }
   ],
   "source": [
    "# generated by LLM\n",
    "\n",
    "class HardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the hard sigmoid activation function as described in the paper:\n",
    "        σ_H(x) = 0 if x <= 0\n",
    "               = x if 0 < x < 1\n",
    "               = 1 if x >= 1\n",
    "    This is equivalent to: torch.clamp(x, min=0.0, max=1.0)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Clamp values between 0 and 1\n",
    "        return torch.clamp(x, min=0.0, max=1.0)\n",
    "\n",
    "\n",
    "class LowerLeakyHardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Lower-leaky hard sigmoid: σH,lower(x)\n",
    "    - 0.01*x if x <= 0 (leaky below 0)\n",
    "    - x if 0 <= x <= 1 (linear in middle)  \n",
    "    - 1 if x >= 1 (saturated above 1)\n",
    "    \n",
    "    Used for forward pass masks in stochastic reconstruction losses.\n",
    "    \"\"\"\n",
    "    def __init__(self, leak_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.leak_slope = leak_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(\n",
    "            x <= 0, \n",
    "            self.leak_slope * x,\n",
    "            torch.where(\n",
    "                x >= 1,\n",
    "                torch.ones_like(x),\n",
    "                x\n",
    "            )\n",
    "        )\n",
    "\n",
    "class UpperLeakyHardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Upper-leaky hard sigmoid: σH,upper(x)  \n",
    "    - 0 if x <= 0 (hard cutoff below 0)\n",
    "    - x if 0 <= x <= 1 (linear in middle)\n",
    "    - 1 + 0.01*(x-1) if x >= 1 (leaky above 1)\n",
    "    \n",
    "    Used for importance loss computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, leak_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.leak_slope = leak_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(\n",
    "            x <= 0,\n",
    "            torch.zeros_like(x), \n",
    "            torch.where(\n",
    "                x >= 1,\n",
    "                1 + self.leak_slope * (x - 1),\n",
    "                x\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Test the functions\n",
    "if __name__ == \"__main__\":\n",
    "    lower_leaky = LowerLeakyHardSigmoid()\n",
    "    upper_leaky = UpperLeakyHardSigmoid()\n",
    "    \n",
    "    test_vals = torch.tensor([-2.0, -0.5, 0.0, 0.3, 0.7, 1.0, 1.5, 2.0])\n",
    "    \n",
    "    print(\"Input:\", test_vals)\n",
    "    print(\"Lower-leaky:\", lower_leaky(test_vals))\n",
    "    print(\"Upper-leaky:\", upper_leaky(test_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50310679-3db3-4b7d-8a8b-24c32dc176e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices:  cpu cpu\n",
      "Training on device cpu\n",
      "Starting epoch 1, lr = 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:   1%|▋                                                                                      | 1/118 [00:01<02:48,  1.44s/it, loss=2.05e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_imp min/max: tensor(-0.8395) tensor(0.6943)\n",
      "mask min/max: tensor(-0.0004) tensor(0.9995)\n",
      "Masked out min:  -0.5641949772834778 , max:  0.5537863373756409\n",
      "Faithfulness: 0.0018912713276222348, Stoch Rec: 723.1766967773438, Stoch Rec Layerwise: 1329.784912109375, Importance Min: 12.386624336242676\n",
      "Total gradient norm: 2942.6653772050327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|████████████████████████████████████████████████████████████████████████████████████████| 118/118 [02:50<00:00,  1.44s/it, loss=66.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0 0.0\n",
      "Starting epoch 2, lr = 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25:   1%|▊                                                                                          | 1/118 [00:01<02:46,  1.42s/it, loss=337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_imp min/max: tensor(-0.5785) tensor(0.9342)\n",
      "mask min/max: tensor(-0.0037) tensor(0.9999)\n",
      "Masked out min:  -0.5530452132225037 , max:  1.2272255420684814\n",
      "Faithfulness: 0.0017594245728105307, Stoch Rec: 183.82205200195312, Stoch Rec Layerwise: 150.7386932373047, Importance Min: 22.472126007080078\n",
      "Total gradient norm: 571.0275854293837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25:  36%|████████████████████████████████                                                          | 42/118 [01:00<01:53,  1.49s/it, loss=256]"
     ]
    }
   ],
   "source": [
    "# generated by LLM\n",
    "\n",
    "class HardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the hard sigmoid activation function as described in the paper:\n",
    "        σ_H(x) = 0 if x <= 0\n",
    "               = x if 0 < x < 1\n",
    "               = 1 if x >= 1\n",
    "    This is equivalent to: torch.clamp(x, min=0.0, max=1.0)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Clamp values between 0 and 1\n",
    "        return torch.clamp(x, min=0.0, max=1.0)\n",
    "\n",
    "\n",
    "class LowerLeakyHardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Lower-leaky hard sigmoid: σH,lower(x)\n",
    "    - 0.01*x if x <= 0 (leaky below 0)\n",
    "    - x if 0 <= x <= 1 (linear in middle)  \n",
    "    - 1 if x >= 1 (saturated above 1)\n",
    "    \n",
    "    Used for forward pass masks in stochastic reconstruction losses.\n",
    "    \"\"\"\n",
    "    def __init__(self, leak_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.leak_slope = leak_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(\n",
    "            x <= 0, \n",
    "            self.leak_slope * x,\n",
    "            torch.where(\n",
    "                x >= 1,\n",
    "                torch.ones_like(x),\n",
    "                x\n",
    "            )\n",
    "        )\n",
    "\n",
    "class UpperLeakyHardSigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Upper-leaky hard sigmoid: σH,upper(x)  \n",
    "    - 0 if x <= 0 (hard cutoff below 0)\n",
    "    - x if 0 <= x <= 1 (linear in middle)\n",
    "    - 1 + 0.01*(x-1) if x >= 1 (leaky above 1)\n",
    "    \n",
    "    Used for importance loss computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, leak_slope=0.01):\n",
    "        super().__init__()\n",
    "        self.leak_slope = leak_slope\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(\n",
    "            x <= 0,\n",
    "            torch.zeros_like(x), \n",
    "            torch.where(\n",
    "                x >= 1,\n",
    "                1 + self.leak_slope * (x - 1),\n",
    "                x\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class SPDModelMLP(nn.Module): \n",
    "    # fix the betas stuff below that's basically a type hint\n",
    "    def __init__(self, target_model, config, device=\"cpu\"): \n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"Devices: \", device, target_model.device)\n",
    "        self.device = device\n",
    "        object.__setattr__(self, \"target_model\", target_model) # sets pointer to target_model without registering its parameters as subsidiary\n",
    "        assert self.device == target_model.device, \"Models not on same device\"\n",
    "\n",
    "        self.num_layers, self.pre_embed_size, self.in_size, self.hidden_size, self.imp_hidden_size = config[\"num_layers\"], config[\"pre_embed_size\"], config[\"in_size\"], config[\"hidden_size\"], config[\"importance_mlp_size\"]\n",
    "        self.C = config[\"subcomponents_per_layer\"]\n",
    "        self.hypers = dict(list(config.items())[4:]) # sets the \"hypers\" to contain all the hyperparameters for the model\n",
    "        \n",
    "        # Subcomponent vectors, each of shape C by in_size; to be used\n",
    "        # with outer product to create our low-rank subcomponent matrices\n",
    "        self.V_embed = nn.Parameter(torch.empty((self.C, self.pre_embed_size,), device=device))\n",
    "        self.U_embed = nn.Parameter(torch.empty((self.C, self.in_size,), device=device))\n",
    "        self.V_unembed = nn.Parameter(torch.empty((self.C, self.in_size,), device=device))\n",
    "        self.U_unembed = nn.Parameter(torch.empty((self.C, self.pre_embed_size,), device=device))\n",
    "\n",
    "        self.V_in = nn.ParameterList([torch.empty((self.C, self.in_size,), device=device) for i in range(self.num_layers)])\n",
    "        self.U_in = nn.ParameterList([torch.empty((self.C, self.hidden_size,), device=device) for i in range(self.num_layers)]) \n",
    "        self.V_out = nn.ParameterList([torch.empty((self.C, self.hidden_size,), device=device) for i in range(self.num_layers)])\n",
    "        self.U_out = nn.ParameterList([torch.empty((self.C, self.in_size,), device=device) for i in range(self.num_layers)])        \n",
    "        \n",
    "        # idk what you do with the biases lol\n",
    "        self.b = nn.ParameterList([torch.zeros((self.hidden_size,), device=device) for i in range(self.num_layers)])\n",
    "        \n",
    "        # this is so horrible I'm sorry\n",
    "        # gate_in_in is the gate_in weights for the in subcomponent of each layer\n",
    "        # gate_in_out is gate_in weights for the out component\n",
    "        # they each get an extra one on the end which is for the embed matrix\n",
    "        # Probably should have just registered these as submodels, but I didn't know that those existed before.\n",
    "        self.imp_W_gate_in_in = nn.ParameterList([torch.empty((self.C, 1, self.imp_hidden_size), device=device) for i in range(self.num_layers+1)])\n",
    "        self.imp_W_gate_out_in = nn.ParameterList([torch.empty((self.C, self.imp_hidden_size, 1), device=device) for i in range(self.num_layers+1)]) \n",
    "        self.imp_b_in_in = nn.ParameterList([torch.empty((self.C, self.imp_hidden_size), device=device) for i in range(self.num_layers+1)])\n",
    "        self.imp_b_out_in = nn.ParameterList([torch.empty((self.C, 1), device=device) for i in range(self.num_layers+1)])\n",
    "        \n",
    "        self.imp_W_gate_in_out = nn.ParameterList([torch.empty((self.C, 1, self.imp_hidden_size), device=device) for i in range(self.num_layers+1)])\n",
    "        self.imp_W_gate_out_out = nn.ParameterList([torch.empty((self.C, self.imp_hidden_size, 1), device=device) for i in range(self.num_layers+1)]) \n",
    "        self.imp_b_in_out = nn.ParameterList([torch.empty((self.C, self.imp_hidden_size), device=device) for i in range(self.num_layers+1)])\n",
    "        self.imp_b_out_out = nn.ParameterList([torch.empty((self.C, 1), device=device) for i in range(self.num_layers+1)])\n",
    "        \n",
    "        # self.pred_weights = torch.empty((num_layers, self.C), device=device)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            # using xavier anyway -- note that the variance etc is\n",
    "            # changed because we take the outer product in the \n",
    "            # forward pass\n",
    "            if param.dim() >= 2: # xavier does not work for 1d tensors\n",
    "                nn.init.xavier_normal_(param)\n",
    "            else:\n",
    "                nn.init.zeros_(param)\n",
    "                param = param + 0.1\n",
    "\n",
    "        \n",
    "    def forward(self, x, return_activs_and_weights=False, masks=None): # Regular run. Unclear whether I should have masking when I do a regular forward pass.\n",
    "        v_activations = []\n",
    "        weight_matrices = []\n",
    "        x_in = x.clone()\n",
    "        layerwise_resids = []\n",
    "\n",
    "        if masks is not None: \n",
    "            \n",
    "            v_activ_embed = torch.einsum(\"np,cp->nc\", x_in, self.V_embed) * masks[-1][\"embed\"]\n",
    "            v_activ_embed_nomask = torch.einsum(\"np,cp->nc\", x_in, self.V_embed)\n",
    "\n",
    "            x_embedded = torch.einsum(\"nc,ci->ni\", v_activ_embed, self.U_embed)\n",
    "            x_embedded_nomask = torch.einsum(\"nc,ci->ni\", v_activ_embed_nomask, self.U_embed)\n",
    "\n",
    "            x_resid = x_embedded.clone()\n",
    "                        \n",
    "            for l in range(self.num_layers):\n",
    "                ### Run the forward pass just for this layer\n",
    "                v_activ_layer_in = torch.einsum(\"ni,ci->nc\", x_resid, self.V_in[l]) * masks[l][\"in\"] # V activations\n",
    "                layer_hidden = F.relu(torch.einsum(\"nc,ch->nh\", v_activ_layer_in, self.U_in[l]) + self.b[l]) # U and ReLU\n",
    "                v_activ_layer_out = torch.einsum(\"nh,ch->nc\", layer_hidden, self.V_out[l]) * masks[l][\"out\"] # V activ\n",
    "                layer_out = torch.einsum(\"nc,ci->ni\", v_activ_layer_out, self.U_out[l]) # U\n",
    "                x_resid = x_resid + layer_out # add to residual stream\n",
    "\n",
    "                # Run the layerwise forward pass for the whole model, with masking only this layer\n",
    "                x_resid_layerwise_in = x_embedded_nomask.clone() # pre-embedded\n",
    "                x_resid_layerwise_out = x_embedded_nomask.clone() # pre-embedded again\n",
    "                \n",
    "                for l_2 in range(self.num_layers): # oh god this is so awful. literally sobbing rn. i can't believe I'm writing it like this\n",
    "                    if l_2 == l: \n",
    "                        # run with mask on in\n",
    "                        v_activ_layer_in_l = torch.einsum(\"ni,ci->nc\", x_resid_layerwise_in, self.V_in[l]) * masks[l][\"in\"]\n",
    "                        layer_hidden_l = F.relu(torch.einsum(\"nc,ch->nh\", v_activ_layer_in_l, self.U_in[l])+self.b[l])\n",
    "                        v_activ_layer_out_l = torch.einsum(\"nh,ch->nc\", layer_hidden_l, self.V_out[l])\n",
    "                        layer_out_l_masked_in = torch.einsum(\"nc,ci->ni\", v_activ_layer_out_l, self.U_out[l])\n",
    "                        x_resid_layerwise_in = x_resid_layerwise_in + layer_out_l_masked_in\n",
    "\n",
    "                        # run with mask on out\n",
    "                        v_activ_layer_in_l = torch.einsum(\"ni,ci->nc\", x_resid_layerwise_out, self.V_in[l])\n",
    "                        layer_hidden_l = F.relu(torch.einsum(\"nc,ch->nh\", v_activ_layer_in_l, self.U_in[l])+self.b[l])\n",
    "                        v_activ_layer_out_l = torch.einsum(\"nh,ch->nc\", layer_hidden_l, self.V_out[l]) * masks[l][\"out\"]\n",
    "                        layer_out_l_masked_out = torch.einsum(\"nc,ci->ni\", v_activ_layer_out_l, self.U_out[l])\n",
    "                        x_resid_layerwise_out = x_resid_layerwise_out + layer_out_l_masked_out\n",
    "\n",
    "                        \n",
    "                    else: # THE SAME EXACT THING EXCEPT WITHOUT THE MASKS >_< \n",
    "                        v_activ_layer_in_l = torch.einsum(\"ni,ci->nc\", x_resid_layerwise_in, self.V_in[l])\n",
    "                        layer_hidden_l = F.relu(torch.einsum(\"nc,ch->nh\", v_activ_layer_in_l, self.U_in[l]) + self.b[l])\n",
    "                        v_activ_layer_out_l = torch.einsum(\"nh,ch->nc\", layer_hidden_l, self.V_out[l])\n",
    "                        layer_out_l = torch.einsum(\"nc,ci->ni\", v_activ_layer_out_l, self.U_out[l])\n",
    "                        x_resid_layerwise_in = x_resid_layerwise_in + layer_out_l\n",
    "                        \n",
    "                        v_activ_layer_in_l = torch.einsum(\"ni,ci->nc\", x_resid_layerwise_out, self.V_in[l])\n",
    "                        layer_hidden_l = F.relu(torch.einsum(\"nc,ch->nh\", v_activ_layer_in_l, self.U_in[l]) + self.b[l])\n",
    "                        v_activ_layer_out_l = torch.einsum(\"nh,ch->nc\", layer_hidden_l, self.V_out[l])\n",
    "                        layer_out_l = torch.einsum(\"nc,ci->ni\", v_activ_layer_out_l, self.U_out[l])\n",
    "                        x_resid_layerwise_out = x_resid_layerwise_out + layer_out_l\n",
    "                        \n",
    "                    x_out_layerwise_in = torch.einsum(\"nc,cp->np\", torch.einsum(\"ni,ci->nc\", x_resid_layerwise_in, self.V_unembed), self.U_unembed)\n",
    "                    x_out_layerwise_out = torch.einsum(\"nc,cp->np\", torch.einsum(\"ni,ci->nc\", x_resid_layerwise_out, self.V_unembed), self.U_unembed)\n",
    "                    layerwise_resids.append({\"in\":x_out_layerwise_in,\"out\":x_out_layerwise_out})\n",
    "                    \n",
    "                # run layerwise for embed and unembed as well, because very intelligently I gave them their own separate weight matrices \n",
    "                # and they don't get included when iterating through layers\n",
    "                x_embed_resid = x_embedded.clone()\n",
    "                for l in range(self.num_layers):\n",
    "                    v_activ_layer_in_l = torch.einsum(\"ni,ci->nc\", x_embed_resid, self.V_in[l])\n",
    "                    layer_hidden_l = F.relu(torch.einsum(\"nc,ch->nh\", v_activ_layer_in_l, self.U_in[l]) + self.b[l])\n",
    "                    v_activ_layer_out_l = torch.einsum(\"nh,ch->nc\", layer_hidden_l, self.V_out[l])\n",
    "                    layer_out_embed_mask = torch.einsum(\"nc,ci->ni\", v_activ_layer_out_l, self.U_out[l])\n",
    "                    x_embed_resid = x_embed_resid + layer_out_embed_mask\n",
    "                x_out_embed_mask = torch.einsum(\"nc,cp->np\", torch.einsum(\"ni,ci->nc\", x_embed_resid, self.V_unembed), self.U_unembed)\n",
    "\n",
    "                x_unembed_resid = x_embedded_nomask.clone()\n",
    "                for l in range(self.num_layers):\n",
    "                    v_activ_layer_in_l = torch.einsum(\"ni,ci->nc\", x_unembed_resid, self.V_in[l])\n",
    "                    layer_hidden_l = F.relu(torch.einsum(\"nc,ch->nh\", v_activ_layer_in_l, self.U_in[l]) + self.b[l])\n",
    "                    v_activ_layer_out_l = torch.einsum(\"nh,ch->nc\", layer_hidden_l, self.V_out[l])\n",
    "                    layer_out_unembed_mask = torch.einsum(\"nc,ci->ni\", v_activ_layer_out_l, self.U_out[l])\n",
    "                    x_unembed_resid = x_unembed_resid + layer_out_unembed_mask\n",
    "                x_out_unembed_mask = torch.einsum(\"nc,cp->np\", torch.einsum(\"ni,ci->nc\", x_unembed_resid, self.V_unembed)* masks[-1][\"unembed\"], self.U_unembed)\n",
    "\n",
    "                layerwise_resids.append({\"embed\": x_out_embed_mask, \"unembed\": x_out_unembed_mask})\n",
    "\n",
    "            v_activ_unembed = torch.einsum(\"ni,ci->nc\", x_resid, self.V_unembed) * masks[-1][\"unembed\"]\n",
    "            x_out = torch.einsum(\"nc,cp->np\", v_activ_unembed, self.U_unembed)\n",
    "\n",
    "                                \n",
    "        else: \n",
    "            W_embed = torch.einsum(\"cp, ci -> cpi\", self.V_embed, self.U_embed).sum(dim=0)\n",
    "            x_resid = torch.einsum(\"np, pi -> ni\", x_in, W_embed)\n",
    "            v_activ_embed = torch.einsum(\"np,cp->nc\", x_in, self.V_embed).unsqueeze(-1)\n",
    "            \n",
    "            for l in range(self.num_layers):\n",
    "                # Use outer product to create weights for the layer, then sum all the subcomponents\n",
    "                W_in = torch.einsum(\"ci,ch-> cih\", self.V_in[l], self.U_in[l]).sum(dim=0) # shape i h\n",
    "                W_out = torch.einsum(\"ch,ci-> chi\", self.V_out[l], self.U_out[l]).sum(dim=0) # shape h i\n",
    "                if return_activs_and_weights == True:\n",
    "                    weight_matrices.append({\"in\": W_in, \"out\": W_out})\n",
    "                # COMPUTE \n",
    "                layer_in = F.relu(torch.einsum(\"ni,ih -> nh\", x_resid, W_in) + self.b[l])\n",
    "                layer_out = torch.einsum(\"nh,hi->ni\", F.relu(layer_in), W_out)\n",
    "            \n",
    "                if return_activs_and_weights == True: # LMAO I HAD IT INDENTED WRONG\n",
    "                    # calculate activations\n",
    "                    v_activ_layer_in = torch.einsum(\"ni,ci->nc\", x_resid, self.V_in[l]).unsqueeze(-1)\n",
    "                    v_activ_layer_out = torch.einsum(\"nh,ch->nc\", layer_in, self.V_out[l]).unsqueeze(-1)\n",
    "                    # both are now shape n,c,1\n",
    "                    v_activations.append({\"in\": v_activ_layer_in, \"out\": v_activ_layer_out})\n",
    "                    \n",
    "                    # code to make sure I'm not doing some goofy shit with the activations at the beginning\n",
    "                    check_layer_in_uv = F.relu(torch.einsum(\"nc,ch->nh\", v_activ_layer_in.squeeze(), self.U_in[l]) + self.b[l])\n",
    "                    assert torch.allclose(layer_in, check_layer_in_uv, atol=10), f\"Subcomponent activations not calculated correctly, max difference is {torch.max(layer_in-check_layer_in_uv)}; {layer_in}{check_layer_in_uv}\"\n",
    "                    # would in theory have one for the second too but I'm being lazy\n",
    "                \n",
    "            x_resid = x_resid + layer_out\n",
    "            W_unembed = torch.einsum(\"ci, cp-> cip\", self.V_unembed, self.U_unembed).sum(dim=0)\n",
    "            x_out = torch.einsum(\"ni, ip -> np\", x_resid, W_unembed)\n",
    "            \n",
    "            if return_activs_and_weights == True:\n",
    "                weight_matrices.append({\"embed\": W_embed, \"unembed\": W_unembed})\n",
    "                v_activ_unembed = torch.einsum(\"ni,ci->nc\", x_resid, self.V_unembed).unsqueeze(-1)\n",
    "                v_activations.append({\"embed\": v_activ_embed, \"unembed\": v_activ_unembed})\n",
    "            # putting the embed matrices at the end of the masks. since we iterate through l in range(num_layers) this will not get added! :D\n",
    "\n",
    "        if return_activs_and_weights:\n",
    "            return x_out, v_activations, weight_matrices \n",
    "        elif masks is not None: \n",
    "            return x_out, layerwise_resids\n",
    "        else: \n",
    "            return x_out\n",
    "            \n",
    "loss_history = []\n",
    "loss_history_stoch_rec = []\n",
    "loss_history_stoch_rec_layer = []\n",
    "loss_history_faithfulness = []\n",
    "loss_history_imp_min = []\n",
    "\n",
    "\n",
    "def train_SPD(spd_model, dataloader, lr=1e-5, num_epochs=20):\n",
    "\n",
    "    model.train()\n",
    "    print(f\"Training on device {spd_model.device}\")\n",
    "    optimizer = torch.optim.AdamW(spd_model.parameters(), lr = lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, \n",
    "        step_size=4,   # e.g. every 4 epochs\n",
    "        gamma=0.5      # multiply LR by 0.5 each time\n",
    "    )\n",
    "    \n",
    "    P = sum(p.numel() for p in spd_model.target_model.parameters())\n",
    "\n",
    "    upper_leaky_sigmoid = UpperLeakyHardSigmoid()\n",
    "    lower_leaky_sigmoid = LowerLeakyHardSigmoid()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_l_stoch_rec, total_l_stoch_rec_l, total_l_imp, total_l_faith = 0.0, 0.0, 0.0, 0.0\n",
    "        print(f\"Starting epoch {epoch+1}, lr = {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        with tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as t:\n",
    "            for batch_idx, (x,y) in enumerate(t):                \n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                N = x.shape[0]     # x is shape N by in_size\n",
    "                C = spd_model.C\n",
    "                target_model = spd_model.target_model\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # COMPUTE TARGET OUTPUT\n",
    "                with torch.no_grad(): \n",
    "                    target_out = spd_model.target_model(x)\n",
    "            \n",
    "                # FAITHFULNESS LOSS\n",
    "                \n",
    "                spd_output, spd_activations, spd_weights = spd_model(x, return_activs_and_weights = True)\n",
    "                squared_error = 0\n",
    "                \n",
    "                embed_diff = target_model.W_embed - spd_weights[-1][\"embed\"]\n",
    "                unembed_diff = target_model.W_unembed - spd_weights[-1][\"unembed\"]\n",
    "                squared_error = squared_error + torch.linalg.matrix_norm(embed_diff) ** 2 + torch.linalg.matrix_norm(unembed_diff) ** 2 \n",
    "                \n",
    "                for l in range(spd_model.num_layers):\n",
    "                    in_diff = target_model.W_in[l] - spd_weights[l][\"in\"]\n",
    "                    out_diff = target_model.W_out[l] - spd_weights[l][\"out\"]\n",
    "                    \n",
    "                    # torch.linalg.matrix_norm defaults to the frobenius norm\n",
    "                    # this takes the frobenius norm of the diff and then squares it\n",
    "                    squared_error_layer = torch.linalg.matrix_norm(in_diff) ** 2 + torch.linalg.matrix_norm(out_diff) ** 2 \n",
    "                    squared_error = squared_error + squared_error_layer\n",
    "                    \n",
    "                mean_squared_error = squared_error/P\n",
    "                \n",
    "                l_faithfulness = mean_squared_error\n",
    "                \n",
    "            \n",
    "                ## IMPORTANCE-MINIMALITY LOSS\n",
    "                \n",
    "                pred_importances = []\n",
    "                l_importance_minimality = 0.0\n",
    "\n",
    "                components_imp_pred_embed_hidden = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[-1][\"embed\"], spd_model.imp_W_gate_in_in[-1]) + spd_model.imp_b_in_in[-1])\n",
    "                components_imp_pred_embed = (torch.einsum(\"ncs,cso->nco\", components_imp_pred_embed_hidden, spd_model.imp_W_gate_out_in[-1]) + spd_model.imp_b_out_in[-1])\n",
    "                \n",
    "                components_imp_pred_unembed_hidden = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[-1][\"unembed\"], spd_model.imp_W_gate_in_out[-1]) + spd_model.imp_b_in_out[-1])\n",
    "                components_imp_pred_unembed = (torch.einsum(\"ncs,cso->nco\", components_imp_pred_unembed_hidden, spd_model.imp_W_gate_out_out[-1]) + spd_model.imp_b_out_out[-1])\n",
    "                l_importance_minimality = l_importance_minimality + (upper_leaky_sigmoid(components_imp_pred_embed).sum() ** spd_model.hypers[\"causal_imp_min\"]) + (upper_leaky_sigmoid(components_imp_pred_unembed).sum() ** spd_model.hypers[\"causal_imp_min\"])\n",
    "            \n",
    "                for l in range(spd_model.num_layers):\n",
    "                    # both activations are n by c containing dot product so we already have hard_sigmoid\n",
    "                    # spd_activations[l][inout] is shape n,c,1 (nco)\n",
    "                    # imp_W_in is c by 1 by imp_size (cos)\n",
    "                    # want to map to ncs then back to nco\n",
    "                    # imp_b_in is shape (C, s) so should broadcast to ncs nicely\n",
    "                    \n",
    "                    # TODO: DEFINE hard_sigmoid AS A TORCH MODULE SO IT CAN CALCULATE THE DERIVATIVE \n",
    "                    # in theory should write this as a bunch of models stored in the main model, but that's not how i did it and i've already \n",
    "                    # written this, shrug\n",
    "                    # oh my god this is so bad \n",
    "                    # imp_W_gate_in_in is the gate in for the W_in, etc\n",
    "                    components_imp_pred_hidden_in = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[l][\"in\"], spd_model.imp_W_gate_in_in[l]) + spd_model.imp_b_in_in[l])\n",
    "                    components_pred_layer_in = torch.einsum(\"ncs,cso->nco\", components_imp_pred_hidden_in, spd_model.imp_W_gate_out_in[l]) + spd_model.imp_b_out_in[l]\n",
    "            \n",
    "                    #same thing for the out matrix in layer l\n",
    "                    components_imp_pred_hidden_out = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[l][\"out\"], spd_model.imp_W_gate_in_out[l]) + spd_model.imp_b_in_out[l])\n",
    "                    components_pred_layer_out = (torch.einsum(\"ncs,cso->nco\", components_imp_pred_hidden_out, spd_model.imp_W_gate_out_out[l]) + spd_model.imp_b_out_out[l])\n",
    "                    \n",
    "                    pred_importances.append({\"in\": components_pred_layer_in, \"out\": components_pred_layer_out})\n",
    "                    l_importance_minimality = l_importance_minimality + (upper_leaky_sigmoid(components_pred_layer_in).sum() ** spd_model.hypers[\"causal_imp_min\"]) + (upper_leaky_sigmoid(components_pred_layer_out).sum() ** spd_model.hypers[\"causal_imp_min\"])\n",
    "                \n",
    "                pred_importances.append({\"embed\": components_imp_pred_embed, \"unembed\": components_imp_pred_unembed})\n",
    "                \n",
    "                \n",
    "                l_importance_minimality /= N # divide by N, the batch size, to avg across the batch (notated as B in the paper)\n",
    "            \n",
    "            \n",
    "                ## STOCHASTIC RECONSTRUCTION LOSS \n",
    "            \n",
    "                l_stochastic_recon = 0.0\n",
    "                l_stochastic_recon_layerwise = 0.0\n",
    "                R = torch.rand((spd_model.hypers[\"num_mask_samples\"], N, spd_model.num_layers+1, 2, C), device=device)\n",
    "            \n",
    "                for s in range(spd_model.hypers[\"num_mask_samples\"]):\n",
    "                    layer_masks = []\n",
    "                    # Running this with a for loop. This is slow; I'd ideally just run \n",
    "                    # something like M = G[None, :, :] + (1 - G[None, :, :]) * R\n",
    "                    # But I think there's a clarity tradeoff here so I'm just going to do this for now\n",
    "                    layer_mask_embed = lower_leaky_sigmoid(pred_importances[-1][\"embed\"].squeeze()) + (torch.ones_like(pred_importances[-1][\"embed\"].squeeze()) - lower_leaky_sigmoid(pred_importances[-1][\"embed\"].squeeze()))*R[s,:,spd_model.num_layers,0,:]\n",
    "                    layer_mask_unembed = lower_leaky_sigmoid(pred_importances[-1][\"unembed\"].squeeze()) + (torch.ones_like(pred_importances[-1][\"unembed\"].squeeze()) - lower_leaky_sigmoid(pred_importances[-1][\"unembed\"].squeeze())) * R[s,:,spd_model.num_layers,1,:]\n",
    "                    \n",
    "                    for l in range(spd_model.num_layers):\n",
    "                        # ugh, this is pretty bad. Ideally I'd go back and refactor so that I don't \n",
    "                        # have separate in and out weights, but this is what I have. going to keep it\n",
    "                        # like this for now, but will refactor once I have something that works.\n",
    "                        # components pred in is shape N, C, 1, squeeze to N, C. there are L of them\n",
    "                        # R is N, L, C, sample just l to get N, C\n",
    "                        # I want masks for each component on each layer on each datapoint, so\n",
    "                        # masks should be one C vector for each layer, per-datapoint. so NLC\n",
    "    \n",
    "                        # whoops i'm now putting squeezes on everything. seems like I should just unsqueeze the thing when I store it, but I don't feel like refactoring it like that :( \n",
    "                        layer_mask_in = lower_leaky_sigmoid(pred_importances[l][\"in\"].squeeze()) + (torch.ones_like(pred_importances[l][\"in\"].squeeze()) - lower_leaky_sigmoid(pred_importances[l][\"in\"].squeeze())) * R[s,:,l,0,:]\n",
    "                        layer_mask_out = lower_leaky_sigmoid(pred_importances[l][\"out\"].squeeze()) + (torch.ones_like(pred_importances[l][\"out\"].squeeze()) - lower_leaky_sigmoid(pred_importances[l][\"out\"].squeeze())) * R[s,:,l,1,:]\n",
    "                        # these are both shape N, C \n",
    "                        layer_masks.append({\"in\": layer_mask_in, \"out\": layer_mask_out})\n",
    "                        \n",
    "                    layer_masks.append({\"embed\": layer_mask_embed, \"unembed\": layer_mask_unembed})\n",
    "                    masked_out, layerwise_masked_outs = spd_model(x, masks=layer_masks)\n",
    "                    l_stochastic_recon = l_stochastic_recon + torch.linalg.matrix_norm(target_out - masked_out) ** 2 # uses matrix norm of difference\n",
    "                    for i in range(len(layerwise_masked_outs)-1):\n",
    "                        l_stochastic_recon_layerwise = l_stochastic_recon_layerwise + torch.linalg.matrix_norm(target_out - layerwise_masked_outs[i][\"in\"]) ** 2 + torch.linalg.matrix_norm(target_out - layerwise_masked_outs[i][\"out\"]) **2\n",
    "                        # l_stochastic_recon_layerwise = l_stochastic_recon_layerwise + (target_out - layerwise_masked_outs[i][\"in\"]) ** 2 + (target_out - layerwise_masked_outs[i][\"out\"]) ** 2 # switch to MSE\n",
    "                        \n",
    "                    l_stochastic_recon_layerwise = l_stochastic_recon_layerwise + torch.linalg.matrix_norm(target_out - layerwise_masked_outs[-1][\"embed\"]) **2 + torch.linalg.matrix_norm(target_out - layerwise_masked_outs[-1][\"unembed\"]) **2\n",
    "                    # l_stochastic_recon_layerwise = l_stochastic_recon_layerwise + (target_out - layerwise_masked_outs[-1][\"embed\"]) ** 2 + (target_out - layerwise_masked_outs[-1][\"unembed\"]) ** 2\n",
    "                \n",
    "                \n",
    "                l_stochastic_recon /= spd_model.hypers[\"num_mask_samples\"]\n",
    "                l_stochastic_recon_layerwise /= (spd_model.hypers[\"num_mask_samples\"] * (2 * spd_model.num_layers + 2) ) #one in/out matrix per layer + embed & unembed\n",
    "                \n",
    "                loss = l_faithfulness + spd_model.hypers[\"beta_1\"] * l_stochastic_recon + spd_model.hypers[\"beta_2\"] * l_stochastic_recon_layerwise + spd_model.hypers[\"beta_3\"] * l_importance_minimality\n",
    "                \n",
    "                loss.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(spd_model.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "                if batch_idx % 118 == 0: \n",
    "                    with torch.no_grad():\n",
    "                        \n",
    "                        G = lower_leaky_sigmoid(pred_importances[0][\"in\"]).squeeze()\n",
    "                        print(\"pred_imp min/max:\", pred_importances[0][\"in\"].min(), pred_importances[0][\"in\"].max())\n",
    "                        print(\"mask min/max:\", layer_masks[0][\"in\"].min(), layer_masks[0][\"in\"].max())\n",
    "\n",
    "                        loss_history.append(l_faithfulness.item())\n",
    "                        loss_history_faithfulness.append(l_faithfulness.item())\n",
    "                        loss_history_stoch_rec.append(l_stochastic_recon.item())\n",
    "                        loss_history_stoch_rec_layer.append(l_stochastic_recon_layerwise.item())\n",
    "                        loss_history_imp_min.append(l_importance_minimality.item())\n",
    "    \n",
    "                        print(\"Masked out min: \", masked_out.min().item(), \", max: \", masked_out.max().item())\n",
    "                        print(f\"Faithfulness: {l_faithfulness}, Stoch Rec: {l_stochastic_recon}, Stoch Rec Layerwise: {l_stochastic_recon_layerwise}, Importance Min: {l_importance_minimality}\")\n",
    "                        total_norm = 0.0\n",
    "                        for p in spd_model.parameters():\n",
    "                            if p.grad is not None:\n",
    "                                param_norm = p.grad.data.norm(2)  # L2 norm of the gradient\n",
    "                                total_norm += param_norm.item() ** 2\n",
    "                        \n",
    "                        total_norm = total_norm ** 0.5\n",
    "                        print(f\"Total gradient norm: {total_norm}\")\n",
    "                    \n",
    "                optimizer.step()\n",
    "    \n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                \n",
    "                t.set_postfix(loss=loss.item())\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        total_l_stoch_rec, total_l_stoch_rec_l, total_l_imp, total_l_faith = total_l_stoch_rec/len(dataloader.dataset), total_l_stoch_rec_l/len(dataloader.dataset), total_l_imp/len(dataloader.dataset), total_l_faith/len(dataloader.dataset)\n",
    "        print(total_l_faith, total_l_stoch_rec, total_l_stoch_rec_l, total_l_imp)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Config\n",
    "    #device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "    config = {\n",
    "        \"num_layers\": 1,\n",
    "        \"pre_embed_size\": 100,\n",
    "        \"in_size\": 1000,\n",
    "        \"hidden_size\": 50,\n",
    "        \"subcomponents_per_layer\": 30, \n",
    "        \"beta_1\": 1.0, \n",
    "        \"beta_2\": 1.0, \n",
    "        \"beta_3\": 0.1, \n",
    "        \"causal_imp_min\": 1.0, \n",
    "        \"num_mask_samples\": 20,\n",
    "        \"importance_mlp_size\": 5,\n",
    "    }\n",
    "        \n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    # Model\n",
    "    spd_model = SPDModelMLP(toy_model, config, \"cpu\")\n",
    "    # Train\n",
    "    train_SPD(spd_model, dataloader, lr=1e-3, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "6731eff5-4248-4332-85c5-ca3afc41f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: \n",
      " tensor([[-0.6508, -0.3835, -0.5334,  ...,  0.2332,  0.6789, -0.0431],\n",
      "        [ 0.8313, -0.0060,  0.2385,  ..., -1.6207, -1.0465,  0.5319],\n",
      "        [-1.4295,  0.5698,  0.3310,  ...,  0.1407,  0.5336,  1.3160],\n",
      "        ...,\n",
      "        [ 0.9585,  0.1153, -0.0409,  ..., -0.6234,  0.2621, -1.8937],\n",
      "        [-0.4140,  0.9382,  2.2324,  ..., -0.6548,  0.4351, -1.5530],\n",
      "        [ 0.3636, -1.8274,  1.4284,  ...,  0.6454, -0.7623, -0.3007]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "SPD Model: \n",
      " tensor([[-0.6506, -0.3855, -0.5378,  ...,  0.2314,  0.6725, -0.0373],\n",
      "        [ 0.8063,  0.0163,  0.2416,  ..., -1.6092, -1.0641,  0.5110],\n",
      "        [-1.4273,  0.5746,  0.3304,  ...,  0.1362,  0.5315,  1.3213],\n",
      "        ...,\n",
      "        [ 0.9615,  0.1235, -0.0454,  ..., -0.6242,  0.2596, -1.8799],\n",
      "        [-0.4194,  0.9413,  2.2452,  ..., -0.6480,  0.4346, -1.5532],\n",
      "        [ 0.3513, -1.8142,  1.4374,  ...,  0.6488, -0.7692, -0.2965]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[1.0811],\n",
      "         [1.0994],\n",
      "         [1.0069],\n",
      "         ...,\n",
      "         [0.9984],\n",
      "         [0.9791],\n",
      "         [0.9810]],\n",
      "\n",
      "        [[1.0620],\n",
      "         [1.0992],\n",
      "         [0.9767],\n",
      "         ...,\n",
      "         [1.0024],\n",
      "         [1.1978],\n",
      "         [0.9712]],\n",
      "\n",
      "        [[1.0814],\n",
      "         [1.0587],\n",
      "         [1.1166],\n",
      "         ...,\n",
      "         [0.9957],\n",
      "         [1.1339],\n",
      "         [0.9674]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0811],\n",
      "         [1.0798],\n",
      "         [1.1075],\n",
      "         ...,\n",
      "         [0.9978],\n",
      "         [0.9779],\n",
      "         [0.9680]],\n",
      "\n",
      "        [[1.0806],\n",
      "         [1.0755],\n",
      "         [1.0091],\n",
      "         ...,\n",
      "         [0.9957],\n",
      "         [1.0236],\n",
      "         [0.9836]],\n",
      "\n",
      "        [[1.0794],\n",
      "         [1.0612],\n",
      "         [1.0098],\n",
      "         ...,\n",
      "         [0.9952],\n",
      "         [1.1441],\n",
      "         [1.0134]]], grad_fn=<AddBackward0>) tensor([[[0.9908],\n",
      "         [0.9813],\n",
      "         [1.1914],\n",
      "         ...,\n",
      "         [1.2376],\n",
      "         [1.0960],\n",
      "         [1.0106]],\n",
      "\n",
      "        [[1.0144],\n",
      "         [0.9813],\n",
      "         [1.3532],\n",
      "         ...,\n",
      "         [0.9030],\n",
      "         [1.1840],\n",
      "         [1.0040]],\n",
      "\n",
      "        [[1.0107],\n",
      "         [0.9826],\n",
      "         [0.8076],\n",
      "         ...,\n",
      "         [1.0118],\n",
      "         [1.0602],\n",
      "         [1.0120]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.9911],\n",
      "         [0.9832],\n",
      "         [0.8425],\n",
      "         ...,\n",
      "         [0.9815],\n",
      "         [1.1256],\n",
      "         [1.0115]],\n",
      "\n",
      "        [[0.9946],\n",
      "         [0.9815],\n",
      "         [1.0421],\n",
      "         ...,\n",
      "         [1.0648],\n",
      "         [1.0877],\n",
      "         [1.0133]],\n",
      "\n",
      "        [[1.0004],\n",
      "         [0.9821],\n",
      "         [0.9485],\n",
      "         ...,\n",
      "         [1.0567],\n",
      "         [1.0545],\n",
      "         [1.0100]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = SparseAutoencoderDataset(in_dim=100, n_samples=100, sparsity=0.9, device=\"cpu\")\n",
    "dataloader = DataLoader(eval_dataset, batch_size=128, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "out_eval = spd_model(batch[0])\n",
    "out_eval_target = spd_model.target_model(batch[0])\n",
    "print(\"Original Model: \\n\", out_eval) \n",
    "print(\"SPD Model: \\n\", out_eval_target)\n",
    "\n",
    "def check_masks(x, spd_model): \n",
    "    pred_importances = []\n",
    "    spd_output, spd_activations, spd_weights = spd_model(x, return_activs_and_weights = True)\n",
    "    components_imp_pred_embed_hidden = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[-1][\"embed\"], spd_model.imp_W_gate_in_in[-1]) + spd_model.imp_b_in_in[-1])\n",
    "    components_imp_pred_embed = (torch.einsum(\"ncs,cso->nco\", components_imp_pred_embed_hidden, spd_model.imp_W_gate_out_in[-1]) + spd_model.imp_b_out_in[-1])\n",
    "    \n",
    "    components_imp_pred_unembed_hidden = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[-1][\"unembed\"], spd_model.imp_W_gate_in_out[-1]) + spd_model.imp_b_in_out[-1])\n",
    "    components_imp_pred_unembed = (torch.einsum(\"ncs,cso->nco\", components_imp_pred_unembed_hidden, spd_model.imp_W_gate_out_out[-1]) + spd_model.imp_b_out_out[-1])\n",
    "    \n",
    "    for l in range(spd_model.num_layers):\n",
    "        components_imp_pred_hidden_in = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[l][\"in\"], spd_model.imp_W_gate_in_in[l]) + spd_model.imp_b_in_in[l])\n",
    "        components_pred_layer_in = torch.einsum(\"ncs,cso->nco\", components_imp_pred_hidden_in, spd_model.imp_W_gate_out_in[l]) + spd_model.imp_b_out_in[l]\n",
    "        components_imp_pred_hidden_out = F.gelu(torch.einsum(\"nco,cos->ncs\", spd_activations[l][\"out\"], spd_model.imp_W_gate_in_out[l]) + spd_model.imp_b_in_out[l])\n",
    "        components_pred_layer_out = (torch.einsum(\"ncs,cso->nco\", components_imp_pred_hidden_out, spd_model.imp_W_gate_out_out[l]) + spd_model.imp_b_out_out[l])\n",
    "        \n",
    "    pred_importances.append({\"in\": components_pred_layer_in, \"out\": components_pred_layer_out})    \n",
    "    pred_importances.append({\"embed\": components_imp_pred_embed, \"unembed\": components_imp_pred_unembed})\n",
    "\n",
    "    return pred_importances\n",
    "\n",
    "importances = check_masks(batch[0], spd_model) \n",
    "print(importances[0][\"in\"], importances[0][\"out\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d7991-1270-4927-873e-168c4e8e9494",
   "metadata": {},
   "source": [
    "## Plotting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b6bbc3-d74b-4172-9774-5fa0e1d69992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for history in [loss_history, loss_history_stoch_rec, loss_history_stoch_rec_layer,loss_history_faithfulness,loss_history_imp_min]: \n",
    "    for idx, item in enumerate(history): \n",
    "        history[idx] = item.item()\n",
    "\"\"\"\n",
    "\n",
    "def plot_loss_histories(loss_history, loss_history_stoch_rec, loss_history_stoch_rec_layer, \n",
    "                       loss_history_faithfulness, loss_history_imp_min, save_interval=10):\n",
    "    \"\"\"\n",
    "    Plot loss histories from saved lists\n",
    "    \n",
    "    Args:\n",
    "        loss_history: List of total losses\n",
    "        loss_history_stoch_rec: List of stochastic reconstruction losses  \n",
    "        loss_history_stoch_rec_layer: List of stochastic reconstruction layer losses\n",
    "        loss_history_faithfulness: List of faithfulness losses\n",
    "        loss_history_imp_min: List of importance min losses\n",
    "        save_interval: How often losses were saved (default 10 batches)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create batch numbers (assuming you save every save_interval batches)\n",
    "    batch_numbers = [(i + 1) * save_interval for i in range(len(loss_history))]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot total loss\n",
    "    ax1.plot(batch_numbers, loss_history, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "    ax1.set_title('Total Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Batch Number')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    if max(loss_history) > 100:  # Use log scale for large values\n",
    "        ax1.set_yscale('log')\n",
    "    \n",
    "    # Plot faithfulness loss\n",
    "    ax2.plot(batch_numbers, loss_history_faithfulness, 'r-', linewidth=2, marker='o', markersize=4)\n",
    "    ax2.set_title('Faithfulness Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Batch Number')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    if max(loss_history_faithfulness) > 100:\n",
    "        ax2.set_yscale('log')\n",
    "    \n",
    "    # Plot stochastic reconstruction loss\n",
    "    ax3.plot(batch_numbers, loss_history_stoch_rec, 'g-', linewidth=2, marker='o', markersize=4)\n",
    "    ax3.set_title('Stochastic Reconstruction Loss', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Batch Number')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot stochastic reconstruction layer loss\n",
    "    ax4.plot(batch_numbers, loss_history_stoch_rec_layer, 'm-', linewidth=2, marker='o', markersize=4)\n",
    "    ax4.set_title('Stochastic Reconstruction Layer Loss', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Batch Number')\n",
    "    ax4.set_ylabel('Loss')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    if max(loss_history_stoch_rec_layer) > 100:\n",
    "        ax4.set_yscale('log')\n",
    "    \n",
    "    # Plot importance min loss\n",
    "    ax5.plot(batch_numbers, loss_history_imp_min, 'c-', linewidth=2, marker='o', markersize=4)\n",
    "    ax5.set_title('Importance Min Loss', fontsize=14, fontweight='bold')\n",
    "    ax5.set_xlabel('Batch Number')\n",
    "    ax5.set_ylabel('Loss')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot all losses together (normalized)\n",
    "    max_vals = [max(loss_history), max(loss_history_faithfulness), max(loss_history_stoch_rec), \n",
    "                max(loss_history_stoch_rec_layer), max(loss_history_imp_min)]\n",
    "    \n",
    "    if all(m > 0 for m in max_vals):  # Only plot if all have valid max values\n",
    "        ax6.plot(batch_numbers, np.array(loss_history)/max(loss_history), 'b-', \n",
    "                linewidth=2, label='Total Loss', marker='o', markersize=3)\n",
    "        ax6.plot(batch_numbers, np.array(loss_history_faithfulness)/max(loss_history_faithfulness), 'r-', \n",
    "                linewidth=2, label='Faithfulness', marker='s', markersize=3)\n",
    "        ax6.plot(batch_numbers, np.array(loss_history_stoch_rec)/max(loss_history_stoch_rec), 'g-', \n",
    "                linewidth=2, label='Stoch Rec', marker='^', markersize=3)\n",
    "        ax6.plot(batch_numbers, np.array(loss_history_stoch_rec_layer)/max(loss_history_stoch_rec_layer), 'm-', \n",
    "                linewidth=2, label='Stoch Rec Layer', marker='d', markersize=3)\n",
    "        ax6.plot(batch_numbers, np.array(loss_history_imp_min)/max(loss_history_imp_min), 'c-', \n",
    "                linewidth=2, label='Importance Min', marker='v', markersize=3)\n",
    "    \n",
    "    ax6.set_title('All Losses (Normalized)', fontsize=14, fontweight='bold')\n",
    "    ax6.set_xlabel('Batch Number')\n",
    "    ax6.set_ylabel('Normalized Loss')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_loss_histories(loss_history, loss_history_stoch_rec, loss_history_stoch_rec_layer, \n",
    "                         loss_history_faithfulness, loss_history_imp_min)\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# In your training loop, save losses every 10 batches like this:\n",
    "if batch_idx % 10 == 0:\n",
    "    loss_history.append(total_loss.item())\n",
    "    loss_history_stoch_rec.append(stoch_rec_loss.item())\n",
    "    loss_history_stoch_rec_layer.append(stoch_rec_layer_loss.item())\n",
    "    loss_history_faithfulness.append(faithfulness_loss.item())\n",
    "    loss_history_imp_min.append(imp_min_loss.item())\n",
    "\n",
    "# Then plot:\n",
    "fig = plot_loss_histories(loss_history, loss_history_stoch_rec, loss_history_stoch_rec_layer, \n",
    "                         loss_history_faithfulness, loss_history_imp_min)\n",
    "\"\"\"\n",
    "\n",
    "# Quick test with dummy data:\n",
    "def test_plotter():\n",
    "    # Create some example data\n",
    "    n_points = 50\n",
    "    batch_nums = range(10, 10 + n_points * 10, 10)\n",
    "    \n",
    "    # Simulate decreasing losses with some noise\n",
    "    loss_history = [1000 * np.exp(-0.1 * i) + np.random.normal(0, 50) for i in range(n_points)]\n",
    "    loss_history_faithfulness = [200 * np.exp(-0.15 * i) + np.random.normal(0, 10) for i in range(n_points)]\n",
    "    loss_history_stoch_rec = [100 + 50 * np.sin(0.3 * i) + np.random.normal(0, 5) for i in range(n_points)]\n",
    "    loss_history_stoch_rec_layer = [500 * np.exp(-0.08 * i) + np.random.normal(0, 20) for i in range(n_points)]\n",
    "    loss_history_imp_min = [10 * np.exp(-0.05 * i) + np.random.normal(0, 1) for i in range(n_points)]\n",
    "    \n",
    "    return plot_loss_histories(loss_history, loss_history_stoch_rec, loss_history_stoch_rec_layer, \n",
    "                              loss_history_faithfulness, loss_history_imp_min)\n",
    "\n",
    "# Uncomment to test:\n",
    "# test_fig = test_plotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "1f69f16b-fd11-4628-b78e-4e92be6543ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ended up with \n",
    "pred_imp min/max: tensor(0.7151) tensor(1.6347)\n",
    "mask min/max: tensor(0.7313) tensor(1.)\n",
    "Masked out min:  -7.006863117218018 , max:  5.271817207336426\n",
    "Faithfulness: 2.7321490847498353e-08, Stoch Rec: 2.3285586833953857, Stoch Rec Layerwise: 1.1707794666290283, Importance Min: 118.15776062011719\n",
    "Total gradient norm: 889.7729806290456\n",
    "\n",
    "on hyperparameters\n",
    "\n",
    "    config = {\n",
    "        \"num_layers\": 1,\n",
    "        \"pre_embed_size\": 100,\n",
    "        \"in_size\": 1000,\n",
    "        \"hidden_size\": 50,\n",
    "        \"subcomponents_per_layer\": 30, \n",
    "        \"beta_1\": 1.0, \n",
    "        \"beta_2\": 1.0, \n",
    "        \"beta_3\": 5e-2, \n",
    "        \"causal_imp_min\": 1.0, \n",
    "        \"num_mask_samples\": 20,\n",
    "        \"importance_mlp_size\": 5,\n",
    "    }\n",
    "\n",
    "\n",
    "ideally should start using wandb lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9471d-1182-429a-911d-2277fda4f4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-replication",
   "language": "python",
   "name": "interp-replication"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
